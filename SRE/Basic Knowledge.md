# SRE Basic Terms

- [SRE Basic Terms](#sre-basic-terms)
  - [REFERENCE](#reference)
  - [NOTE](#note)
  - [Distributed System](#distributed-system)
  - [Containerization](#containerization)
  - [Containers](#containers)
  - [Docker](#docker)
  - [Cloud Native Apps](#cloud-native-apps)
  - [Cluster](#cluster)
  - [Infrastructure as Code (IaC)](#infrastructure-as-code-iac)
  - [Infrastructure as a Service (IaaS)](#infrastructure-as-a-service-iaas)
  - [Kubernetes](#kubernetes)
    - [Kubnernetes API](#kubnernetes-api)
  - [Pod](#pod)
  - [Nodes](#nodes)
  - [Helm](#helm)
  - [Terraform](#terraform)
  - [Terragrunt](#terragrunt)
  - [Multitenancy](#multitenancy)
  - [YAML](#yaml)
  - [ğŸ’¡ K8s Service çš„çœŸå®å«ä¹‰å’Œä½œç”¨](#-k8s-service-çš„çœŸå®å«ä¹‰å’Œä½œç”¨)
    - [æ ¸å¿ƒç—›ç‚¹ä¸ Service çš„è§£å†³æ–¹æ¡ˆ](#æ ¸å¿ƒç—›ç‚¹ä¸-service-çš„è§£å†³æ–¹æ¡ˆ)
  - [ğŸ§ ä¸ºä»€ä¹ˆå«å®ƒ Serviceï¼ˆæœåŠ¡ï¼‰ï¼Ÿ](#-ä¸ºä»€ä¹ˆå«å®ƒ-serviceæœåŠ¡)
  - [ğŸŒ K8s Service çš„ç±»å‹](#-k8s-service-çš„ç±»å‹)
  - [ğŸ·ï¸ 1. Labelï¼ˆæ ‡ç­¾ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ](#ï¸-1-labelæ ‡ç­¾æ˜¯ä»€ä¹ˆ)
    - [ä½œç”¨ï¼šç»„ç»‡å’Œè¯†åˆ«](#ä½œç”¨ç»„ç»‡å’Œè¯†åˆ«)
  - [ğŸ” 2. Selectorï¼ˆé€‰æ‹©å™¨ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ](#-2-selectoré€‰æ‹©å™¨æ˜¯ä»€ä¹ˆ)
    - [ä½œç”¨ï¼šå®šä½å’Œç®¡ç†](#ä½œç”¨å®šä½å’Œç®¡ç†)
    - [å¸¸è§çš„ Selector ä½¿ç”¨åœºæ™¯ï¼š](#å¸¸è§çš„-selector-ä½¿ç”¨åœºæ™¯)
  - [ğŸ”— 3. Label å’Œ Selector çš„å…³ç³»ï¼ˆæ ¸å¿ƒï¼‰](#-3-label-å’Œ-selector-çš„å…³ç³»æ ¸å¿ƒ)
    - [å…³ç³»å›¾ç¤º](#å…³ç³»å›¾ç¤º)

## REFERENCE

[Cloud Native Glossary](https://glossary.cncf.io/)
[What is SRE?](https://www.redhat.com/en/topics/devops/what-is-sre)

## NOTE

- Turn to Helm for packaging applications, Terraform for infrastructure provisioning, and Terragrunt for orchestrating Terraform configurations.
- The combination of Docker, Helm, Terraform, and Terragrunt empowers teams to deploy complex applications to Kubernetes in a streamlined, declarative, and consistent manner.
  - Docker images encapsulate the application code and its dependencies, making deployments portable and reliable. 
  - Helm charts simplify Kubernetes deployments by providing reusable and versioned templates. 
  - Terraform offers a declarative approach to provisioning Kubernetes infrastructure. 
  - Finally, Terragrunt orchestrates the entire process, ensuring consistency and DRY configurations across environments.
- Ideas???
  - Be lazy; automatic everything to avoid toils and to be consistent
  - introduce another layer of abstraction; use these declarative languages; not worry about how these actually do
  - having a single source of truth helps visibility, maintainability and increases durability and/or stability
  - having control over what is going to be deployed is a must-have
- Minikube
  - [Minikub](https://kubernetes.io/docs/tasks/tools/), which offers an excellent way to work with a single-node Kubernetes cluster locally.
- Helm
  - [Helm installation](https://helm.sh/docs/v2/using_helm/#installing-helm)
- Terraform vs Kubernetes
  - Terraform is a tool for provisioning and managing infrastructure as code, such as servers, databases, and networking across multiple cloud providers.
  - Kubernetes is a container orchestration platform that manages the deployment, scaling, and operation of containerized applications.
  - Terraform builds the infrastructure where Kubernetes might run. Kubernetes then manages application workloads on that infrastructure. They often work together but solve different layers of the stack, with Terraform setting up infrastructure and Kubernetes managing application-level operations.
  - Terraform focuses on provisioning infrastructure components and targets the Infrastructure as Code space.
  - Kubernetes aims to enable us to run container workloads and targets the container orchestration space.
  - Kubernetes and Terraform complement each other since they operate at two different levels and can be utilized in parallel.
  - A typical model that cloud practitioners adopt is to use Terraform to provision infrastructure resources (e.g. Kubernetes clusters) and use Kubernetes to manage the containerized apps that run on top of the clusters.
  - Another approach is to use Terraform to manage Kubernetes-specific application components as well. This model has the advantage of adding the Terraform workflow to Kubernetes components. This way, IT operators can detect configuration drift on Kubernetes and manage infrastructure and application resources with the same workflow and configuration language. This approach has a significant disadvantage since Terraform requires a well-defined schema for each managed resource. Thus each Kubernetes resource needs to be translated into a Terraform schema to be available. This dependency makes maintaining Kubernetes resources through Terraform cumbersome at times.
- Terraform vs Helm
  - Infrastructure Provisioning vs. Application Deployment;
    - Terraform excels in provisioning and managing infrastructure resources across various cloud providers. It is suitable for defining and managing virtual machines, Kubernetes clusters, storage, and networking components.
    - Helm, however, focuses on packaging and deploying applications on Kubernetes. It streamlines the installation, upgrading, and rollback of containerized applications.
  - Infrastructure as Code vs. Application Packaging
    - Terraform's strength lies in its ability to define infrastructure resources as code. This approach ensures that infrastructure changes can be version-controlled, reviewed, and shared among teams.
    - Helm, being a package manager, specializes in bundling applications and their configurations into portable packages. This allows for consistent deployment across different environments.
- jife

## Distributed System

A distributed system is a collection of autonomous computing elements connected over a network that appears to users as a single coherent system. Generally referred to as nodes, these components can be hardware devices (e.g. computers, mobile phones) or software processes. Nodes are programmed to achieve a common goal and, to collaborate, they exchange messages over the network.

Distributed systems allow for horizontal scaling (e.g. adding more nodes to the system whenever needed). This can be automated allowing a system to handle a sudden increase in workload or resource consumption.

## Containerization

Containerization is the process of packaging of application code including libraries and dependencies required to run the code into a single lightweight executableâ€”called container image.

Container images are lightweight (unlike traditional VMs) and the containerization process requires a file with a list of dependencies. A container image is stored by a unique identifier that is tied to its exact content and configuration. As containers are scheduled and rescheduled, they are always reset to their initial state which eliminates configuration drift.

a next-generation virtualization and automation solution after Hypervisor Virtualization. It widely applied today because of the breakthrough effect with outstanding advantages in creating and deploying applications faster, more scalable and more securely than traditional methods.

## Containers

A container is a running process with resource and capability constraints managed by a computerâ€™s operating system. The files available to the container process are packaged as a container image. Containers run adjacent to each other on the same machine, but typically the operating system prevents the separate container processes from interacting with each other.

## Docker

a number one software to create, manage and run containers

Encapsulating Application Code for Portability. Docker has become the de facto standard for containerization.

Docker containers have revolutionized software development by encapsulating applications, libraries, and dependencies in a consistent and portable environment. With Docker, you can package your project's application code and all its dependencies into a single, lightweight image. This image can be easily shared, version-controlled, and deployed across different environments without worrying about variations in the underlying infrastructure.

By leveraging Docker images, we can reduce the infamous "it works on my machine" problem. This consistency is especially valuable when deploying applications to Kubernetes clusters, as it eliminates the need to deal with subtle environment differences that could lead to production issues.

Docker is the platform for creating, deploying, and running applications in containers. It provides a consistent environment for applications using containers. It can be used as the container runtime for Kubernetes. 

## Cloud Native Apps

Cloud native applications are specifically designed to take advantage of innovations in cloud computing. These applications integrate easily with their respective cloud architectures, taking advantage of the cloudâ€™s resources and scaling capabilities. It also refers to applications that take advantage of innovations in infrastructure driven by cloud computing. Cloud native applications today include apps that run in a cloud providerâ€™s datacenter and on cloud native platforms on-premise.

## Cluster

A cluster is a group of computers or applications called nodes that work together towards a common goal. In cloud native computing, the term is often applied when talking about a Kubernetes cluster. It can be seen as a specific kind of distributed system where the nodes are a bit more tightly coupled.

## Infrastructure as Code (IaC)

Infrastructure as code is the practice of storing the definition of infrastructure as one or more files. This replaces the traditional model where infrastructure as a service is provisioned manually, usually through shell scripts or other configuration tools

Building applications in a cloud native way requires infrastructure to be disposable and reproducible.

By representing the data center resources such as servers, load balancers, and subnets as code, it allows infrastructure teams to have a single source of truth for all configurations and also allows them to manage their data center in a CI/CD pipeline, implementing version control and deployment strategies.

## Infrastructure as a Service (IaaS)

Infrastructure
Infrastructure as a service, or IaaS, is a cloud computing service model that offers physical or virtualized compute, storage, and network resources on-demand on a pay-as-you-go model. Cloud providers own and operate the hardware and software, available to consumers in public, private, or hybrid cloud deployments.

An on-demand infrastructure allows them to rent compute resources as needed and defer large capital expenditures, or CAPEX, while giving them the flexibility to scale up or down.

IaaS reduces the upfront costs of experimenting or trying a new application and provides facilities to rapidly deploy infrastructure. A cloud provider is an excellent option for development or test environments, which helps developers experiment and innovate.

## Kubernetes

Kubernetes, often abbreviated as K8s, is an open source container orchestrator. It automates the lifecycle of containerized applications on modern infrastructures, functioning as a â€œdatacenter operating systemâ€ that manages applications across a distributed system.

Kubernetes schedules containers across nodes in a cluster, bundling several infrastructure resources such as load balancer, persistent storage, etc. to run containerized applications.

Kubernetes enables automation and extensibility, allowing users to deploy applications declaratively (see below) in a reproducible way.

Similar to traditional infrastructure as code tools, Kubernetes helps with automation but has the advantage of working with containers. Containers are more resistant to configuration drift than virtual or physical machines.

Additionally, Kubernetes works declaratively, which means that instead of operators instructing the machine how to do something, they describe â€” usually as manifest files (e.g., YAML) â€” what the infrastructure should look like. Kubernetes then takes care of the â€œhowâ€. This results in Kubernetes being extremely compatible with infrastructure as code.

Kubernetes (K8s) is an open-source system for container orchestration, automating deployments, and managing containerized apps. Its powerful orchestration system enables applications to scale seamlessly and achieve high availability.

Kubernetes strives to be cloud agnostic at its core, providing great flexibility in running workloads across cloud and on-premises environments.

One of its main benefits is the self-healing capabilities it provides. Containers that fail are automatically restarted and rescheduled, nodes can be configured to be automatically replaced, and traffic is served only by healthy components based on health checks.  

Rollouts are handled progressively, and Kubernetes provides smart mechanisms to monitor application health during deployments. Rolling back problematic changes happens automatically if the application health doesnâ€™t report a healthy status after a new deployment.

Kubernetes handles service discovery and load-balances traffic between similar pods natively without the need for complex external solutions.

Kubernetes provides a cluster of nodes, a group of worker machines that run containerized applications. Each node hosts pods that hold application workload containers. The brain of the whole system is the control plane. Each cluster consists of several components that manage the worker nodes and pods and guarantee operational continuity.

A **pod** is the smallest deployable unit in Kubernetes, encapsulating one or more containers that share storage and network resources. This flexibility is essential for implementing complex applications effectively within a Kubernetes cluster.

Scaling an application in Kubernetes typically involves increasing the number of Pods that run the application, ensuring better resource utilization and improved performance.

[Cluster Architecture](https://kubernetes.io/docs/concepts/architecture/) and [Core Components](https://kubernetes.io/docs/concepts/overview/components/): 

- Control Plane
  - The **API server** is the component exposing the Kubernetes API and operates as the front-end of the control plane by handling all the communication between other parts.
  - The **etcd** (a distributed /etc) component is used to store all cluster data and state. It is a distributed reliable key-value store used by kubernetes to store all configuration and metadata used to manage the cluster.
  - The **scheduler** manages how pods are assigned to nodes and takes all the workload scheduling decisions. It is responsible for distributing work among multiple nodes by deciding which nodes will run specific pods. This function is crucial for optimizing resource utilization and ensuring efficient operation within a Kubernetes cluster. 
  - The **controller manager** components run different controller processes to ensure that the clusterâ€™s desired state matches its current state.
  - The **cloud controller** manager integrates Kubernetes clusters with external cloud providers, embeds their logic, and links the Kubernetes API with the Cloud Providerâ€™s API.
- On each worker node
  - the **kubelet** (the primary "node agent") is the agent responsible for running containers in pods,
  - **kube-proxy** is the component that adds the necessary networking capabilities for communication between pods and nodes.

- kubectl
  - kubectl run; start a pod; deploy an application
  - kubectl cluster-info
  - kubectl get nodes; list all nodes of a cluster; find out how many nodes in this cluster
    - kubectl get nodes - o wide; // get os-image and kernel-version
  - kubectl config view; // show the current Config
  - kubectl get pods --namespace kube-system; // see Helm's Tiller running
  - kubectl describe pod (nginx); // See a pod detail
  - kubectl describe deploy tiller-deploy --namespace=kube-system
  - kubectl apply -f pod.yaml // create(apply) the pod
  - kubectl version // the version of Kubernetes

 **container runtime** is the underlying framework responsible for running applications within containers, such as those created with Docker.
 **kubectl** is the command-line tool specifically designed for managing Kubernetes clusters, allowing you to interact with cluster resources and deploy applications effectively.

### Kubnernetes API

API Server:

- The central component that exposes the Kubernetes API.
- All communication with the cluster, whether from kubectl, other control plane components, or external tools, goes through the API server.

API Objects:

- Everything in Kubernetes is represented as an API object (e.g., Pods, Deployments, Services, ConfigMaps, Secrets, Nodes, Namespaces).
- These objects have a defined schema and state, which is stored in etcd.

API Verbs:

- Standard operations that can be performed on API objects:
- get: Retrieve a specific object.
- list: Retrieve a collection of objects (e.g., all Pods in a Namespace).
- create: Create a new object.
- update: Replace an existing object with a new definition.
- patch: Apply partial updates to an existing object.
- delete: Delete an object.
- watch: Monitor changes to objects in real-time.

API Groups and Versions:

- Kubernetes API objects are organized into API Groups (e.g., apps, batch, networking.k8s.io).
- Each API Group can have multiple versions (e.g., v1, v1beta1). This allows for API evolution and backward compatibility.

Common Interactions:

- Inspecting Resources: kubectl get <resource>, kubectl describe <resource> <name>
- Creating/Updating Resources: kubectl apply -f <manifest.yaml>
- Deleting Resources: kubectl delete <resource> <name>
- Managing Deployments: kubectl rollout status deployment/<name>, kubectl scale deployment/<name> --replicas=<count>
- Accessing Logs: kubectl logs <pod_name>
- Debugging Pods: kubectl exec -it <pod_name> -- <command>, kubectl port-forward pod/<pod_name> <local_port>:<remote_port>

Note: While kubectl provides a convenient command-line interface to interact with the Kubernetes API, direct API interaction can be performed using client libraries in various programming languages (Go, Python, Java, etc.) or by making direct HTTP requests.

## Pod

Within a Kubernetes environment, a pod acts as the most basic deployable unit. It represents an essential building block for deploying and managing containerized applications. Each pod contains a single application instance and can hold one or more containers. Kubernetes manages pods as part of a larger deployment and can scale pods vertically or horizontally as needed.

While containers generally act as independent units that run and control a particular workload, there are cases when containers need to interact and be controlled in a tightly coupled manner.

Pods bundle closely tied containers into a single unit, significantly simplifying container operations. 

Memory and CPU allocation can be defined either on a pod level, allowing the containers inside to share resources in a flexible way, or per container.

## Nodes

A node is a computer that works in concert with other computers, or nodes, to accomplish a common task. Take your laptop, modem, and printer, for example. They are all connected over your wifi network communicating and collaborating, each representing one node. In cloud computing, a node can be a physical computer, a virtual computer, referred to as a VM, or even a container

## Helm

Helm is a package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters. Helm uses a packaging format called charts. A **Helm chart** is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.

Helm, essentially a package manager for Kubernetes, simplifies the deployment process by providing a templating engine and a collection of pre-configured packages called "charts." These charts define the resources needed to run an application in Kubernetes, including deployments, services, and configuration files.

With Helm charts, developers can easily define, version, and manage complex Kubernetes deployments as code.The templating engine allows for parameterization, enabling the reuse of chart configurations with different values for each environment (e.g., development, staging, production). This abstraction helps standardize the deployment process and reduces the chances of human errors when deploying complex applications.

Key Features of Helm:

- Templates: Helm charts are templates for Kubernetes YAML files, which can be customized with variables and flow control statements.
- Package Management: Helm manages Kubernetes applications through Helm charts which support versioning, which makes it easy to roll back to an earlier version if something goes wrong.
- Dependencies: Helm charts can define dependencies on other charts, allowing complex applications to be composed from individual components.

Helm has 4 basic concepts:

- Chart: a collection of YAML files; bundle of the Kubernetes resources needed to build a Kubernetes application. For ease of visualization, Helm Chart can be compared like a Docker Image.
- Config: a configuration in the values.yaml file, which contains configuration explicit to a release of Kubernetes application.
- Release: a chart instance is loaded into Kubernetes. It can be viewed as a version of the Kubernetes application running based on Chart and associated with a specific Config.
- Repositories: a repository of published Charts. These can be private repositories that are only used within the company or public through the Helm Hub. 

Helm has a fairly simple client-server architecture, including a CLI client and an in-cluster server running in the Kubernetes cluster

- **Helm Client**: Provides the developer to use it a command-line interface (CLI) to work with Charts, Config, Release, Repositories. Helm Client will interact with Tiller Server, to perform various actions such as install, upgrade and rollback with Charts, Release.
- **Tiller** Server: an in-cluster server in the Kubernetes cluster, interacting with the Helm Client and communicating with the Kubernetes API server. Thus, Helm can easily manage Kubernetes with tasks such as install, upgrade, query and remove for Kubernetes resources.

Helm focuses on application packaging and deployment. Helm is a Kubernetes package manager that streamlines the installation and management of containerized applications. It uses "charts" as packaged applications, containing all the necessary Kubernetes resources, configuration files, and dependencies. This allows for simplified application deployment, version management, and rollbacks.

A Helm Chart is **a packaging format** used to define, install, upgrade, deploy, share, and manage Kubernetes applications. It lets you group multiple Kubernetes resourcesâ€”like Deployments, Services, and Persistent Volumesâ€”into one reusable unit. Helm Charts automate complex Kubernetes deployments by templating configurations and managing lifecycle upgrades. Charts describe even the most complex apps, provide repeatable application installation, and serve as a single point of authority.

Deploying real-world applications in Kubernetes usually involves several moving parts: app containers, config maps, secrets, service accounts, and storage. Managing all of this manually becomes inefficient fast. Helm solves this by turning infrastructure into modular, repeatable packages. It reduces human error, accelerates CI/CD pipelines, and makes rollbacks as simple as a single command.

A Helm chart describes how to manage a specific application on Kubernetes. It consists of metadata that describes the application, plus the infrastructure needed to operate it in terms of the standard Kubernetes primitives. Each chart references one or more container images that contain the application code to be run.

Despite the fact we can run application containers using the Kubernetes command line (kubectl), the easiest way to run workloads in Kubernetes is using the ready-made Helm charts.

How a Helm Chart Is Organized

- Each Helm Chart is a folder that follows a clear structure, making it easier to manage, update, and reuse across environments.
- **Chart.yaml** â€“ It contains metadata info like the chart name, version, and a brief description.
- **values.yaml** â€“ Defines default configuration values that can be overridden at deployment. the template files collect deployment information from this file. It can customize the helm chart.
- **templates/** â€“ Holds the actual Kubernetes manifest templates using Go templating. stores the actual yaml files. It holds all the configurations for the application. It also includes a tests directory. It use [Golang](https://www.geeksforgeeks.org/go-language/go/) templating format to assemble the configurations from values.yaml or command line and convert the complete configuration into a Kubernetes manifest.
- **charts/** â€“ (Optional) Includes subcharts or external dependencies that the main chart relies on.

Helm terminologies and core components

- Chart: It is a package of containing pre-configured kubernetes resources.
- Release: It is an instance of running charts in a kubernetes resource.
- Repository: It is a collection of charts that can be shared and stored.
- Values: It is useful for configuration settings that can help in customizing chart deployments.
- Helm Chart: It is a helm package manager that contains a collection of kubernetes resource definitions.

Helm is built for scaling, upgrading, and maintaining production environments.

good habits:

- Keep values.yaml files environment-specific (dev, staging, prod)
- Store your charts in version control alongside app code
- Donâ€™t hardcode secretsâ€”use tools like Sealed Secrets or a vault
- Lint charts before deploying with **helm lint**
- Use subcharts for reusable services across apps

commands:

- helm lint: correct any syntax error
- helm template:  replace the placeholder object with original values and show the complete file in console.
- helm install --dry-run: pretend to install the manifests file and try to display the manifests files.
- helm install prod: // kubectl create namespace prod (dev) in advance
- kubectl get all -n prod: confirm the deployment is successful
- helm list: check the number of releases in the host; list all Helm releases.
- helm upgrade (prod)ï¼šif any environment needs to update with image details, versions, replica count, storage memory etcâ€¦, we can do that with upgrade by without bringing down the original state; update a Helm release.
- helm rollback (prod): rollback to previous state
- helm package: package the chart and deploy it to Github, S3, or any chart repository
- helm uninstall (dev): uninstall the helm release; it will remove all of the resources associated with the last release of the chart.
- helm create (mychart): create a chart
- helm version: show both the client and server version
- helm init; // install on MacOs: brew install kubernetes-helm
- helm repo list
- helm repo search (mariadb)
- helm install stable/mariadb
- helm status // ???
- helm delete --deleted (--all)/ ???

For Kubernetes, it is equivalent to yum(Red Hat), apt(Debianã€Ubuntu), or homebrew(macOS, Linux).

A *release* represent an instance of a chart running in a Kubernetes cluster. Each release has its own unique name. 

[In Helm 3, Tiller will be removed](https://helm.sh/blog/helm-3-preview-pt2/), because the tiller inside a K8s cluster has too much poewr such as CREATE/UPDATE/DELETE and it causes some security issues. So, the Helm 3 client library will communicate directly with the Kubernetes API server not via Tiller.

## Terraform

Terraform: Declarative Kubernetes Configurations

While Docker and Helm handle application packaging and Kubernetes deployments, managing the underlying infrastructure can be challenging. Terraform, an Infrastructure as Code (IaC) tool, bridges this gap by providing a declarative way to define, provision, and manage infrastructure resources.

With Terraform's Kubernetes provider, developers can create Kubernetes resources like namespaces, services, and secrets as code. This approach ensures that the infrastructure setup is repeatable, version-controlled, and easily shareable across teams. Additionally, by maintaining the state of the infrastructure, Terraform can automatically manage updates and ensure that the desired state matches the actual state.

Bonus - using the Helm Terraform provider, we can explicitly wire up dependencies between terraform resources, so that the output of a Terraform module can be used as an input to a Helm chart.

Terraform is a tool that allows us to safely and predictably manage infrastructure at scale using cloud-agnostic and infrastructure as code principles. It is a powerful tool developed by Hashicorp that enables infrastructure provisioning both on the cloud and on-premises. 

Terraform is written in a declarative configuration language, Hashicorp Configuration Language (HCL), and facilitates the automation of infrastructure management in any environment.

Modules provide excellent reusability and code-sharing opportunities to boost the collaboration and productivity of teams operating on the cloud. Providers are plugins that offer integration and interaction with different APIs and are one of the main ways to extend Terraformâ€™s functionality. 

Terraform keeps an internal state of the managed infrastructure, which represents resources, configuration, metadata, and their relationships. The state is actively maintained by Terraform and utilized to create plans, track changes, and enable modifications of infrastructure environments. The state should be stored remotely to allow teamwork and collaboration as a best practice.

The core Terraform workflow consists of three concrete stages. First, we generate the infrastructure as code configuration files representing our environmentâ€™s desired state. Next, we check the output of the generated plan based on our manifests. After carefully reviewing the changes, we apply the plan to provision infrastructure resources.

Terraform is an infrastructure provisioning tool that enables users to define and manage their infrastructure as code. It provides a declarative language to describe the desired state of infrastructure resources, such as virtual machines, Kubernetes nodes, storage, and networking components.

Terraform then automates the provisioning and management of these resources across multiple cloud platforms, making it highly versatile. By executing Terraform, the desired infrastructure is automatically provisioned, ensuring consistency and reproducibility.

## Terragrunt

Terragrunt: Orchestrating Terraform Configurations
As infrastructure grows in complexity, managing multiple Terraform configurations across different environments becomes challenging. Terragrunt, an open-source tool, acts as a thin wrapper around Terraform, enabling configuration reuse and better orchestration.

Terragrunt allows developers to follow the Don't Repeat Yourself (DRY) principle by sharing common Terraform configurations across different environments. With Terragrunt's ability to manage dependencies and inheritance, teams can maintain a clear and organized codebase while ensuring consistency and reducing errors.

## Multitenancy

Multitenancy (or multi-tenancy) refers to a single software installation that serves multiple tenants. A **tenant** is a user, application, or a group of users/applications that utilize the software to operate on their own data set. These tenants donâ€™t share data (unless explicitly instructed by the owner) and may not even be aware of one another.

A tenant can be as small as one independent user with a single login ID â€” think personal productivity software â€” or as large as an entire corporation with thousands of login IDs, each with its own privileges yet interrelated in multiple ways.

Without multitenancy, each tenant would need a dedicated software installation. This increases resource utilization and maintenance efforts, ultimately software costs.

Multitenant software provides each tenant a segregated environment (work data, settings, list of credentials, etc.), simultaneously serving multiple tenants. From a tenantâ€™s perspective, each has its dedicated software installation, although, in reality, they are all sharing one.

With multitenant software, tenants share the resources of one installation without affecting each other or only in predefined and controlled ways.

## YAML

Compared with XML and JSON, YAML is much simpler in presenting the same data.

Must have a space after colon ":" in Key-Value Pair. Sibling should have the same dents, but how many spaces do not matter.  

YAML in Kubernetes:

- always contains four keys and they are the root/top properties.
  - apiVersion: v1 of Kubernetes objects, like Pod
  - kind: type of object to create; e.g. Pod, Server, Deployment
  - metadata: data about the object; e.g. name, labels (dict); after labeling, it is easy to group these pods
  - spec: specification on the additional information for the object; depend on the object; a list of containers
- fe

can use some extension or plugin to hight the syntax error or validate the content 

---

æ‚¨æäº†ä¸€ä¸ªéå¸¸å¥½çš„è§‚ç‚¹ï¼ç¡®å®ï¼Œ**"Service"** è¿™ä¸ªè¯åœ¨ IT é¢†åŸŸå¤ªè¿‡é€šç”¨ï¼ˆä¾‹å¦‚ï¼ŒWeb Service, Windows Service, åç«¯æœåŠ¡ç­‰ï¼‰ï¼Œå¾ˆå®¹æ˜“è®©äººå¯¹ Kubernetes ä¸­çš„ `Service` äº§ç”Ÿå›°æƒ‘ã€‚

åœ¨ Kubernetes (K8s) ä¸­ï¼Œ`Service` çš„çœŸå®å«ä¹‰å’Œä½œç”¨ï¼Œä¸æˆ‘ä»¬æ—¥å¸¸è¯´çš„â€œæœåŠ¡â€æœ‰æ‰€ä¸åŒã€‚

---

## ğŸ’¡ K8s Service çš„çœŸå®å«ä¹‰å’Œä½œç”¨

Kubernetes ä¸­çš„ **Service** æ˜¯ä¸€ç§ **æŠ½è±¡å±‚**ï¼Œå®ƒè§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ **Pod çš„ä¸ç¨³å®šæ€§å’Œå‘ç°æ€§**ã€‚

ç®€å•æ¥è¯´ï¼ŒK8s Service æœ‰ä¸¤ä¸ªä¸»è¦èŒè´£ï¼š

1.  **è´Ÿè½½å‡è¡¡å™¨ (Load Balancer):** å°†å®¢æˆ·ç«¯çš„è¯·æ±‚åˆ†æ•£åˆ°å¤šä¸ªåç«¯çš„ Pod ä¸Šã€‚
2.  **ç¨³å®šçš„ç½‘ç»œç«¯ç‚¹ (Stable Network Endpoint):** ä¸ºä¸€ç»„åŠ¨æ€å˜åŒ–çš„ Pod æä¾›ä¸€ä¸ªå›ºå®šçš„ IP åœ°å€å’Œ DNS åç§°ã€‚

### æ ¸å¿ƒç—›ç‚¹ä¸ Service çš„è§£å†³æ–¹æ¡ˆ

| ç—›ç‚¹ | K8s Service å¦‚ä½•è§£å†³ |
| :--- | :--- |
| **Pod æ˜¯ä¸´æ—¶çš„** | Pod éšæ—¶å¯èƒ½å› ä¸ºæ‰©å®¹ã€å‡çº§ã€é‡å¯ã€æ•…éšœç­‰åŸå› è¢«é”€æ¯å¹¶é‡å»ºï¼Œå…¶ IP åœ°å€ä¼šæ”¹å˜ã€‚ |
| **Service æä¾›äº†ç¨³å®šçš„ IP å’Œ DNS åç§°**ã€‚å‰ç«¯åº”ç”¨æˆ–å®¢æˆ·ç«¯ä¸éœ€è¦çŸ¥é“åç«¯ Pod çš„å…·ä½“ IPã€‚ |
| **Pod å‘ç°å›°éš¾** | å®¢æˆ·ç«¯éœ€è¦çŸ¥é“å“ªäº› Pod æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†è¯·æ±‚å‘é€åˆ°å¥åº·çš„ Pod ä¸Šã€‚ |
| **Service å……å½“â€œä»£ç†â€**ã€‚å®ƒä½¿ç”¨ **Selector (æ ‡ç­¾é€‰æ‹©å™¨)** è‡ªåŠ¨è·Ÿè¸ªå’Œç›‘æ§ä¸€ç»„ Podï¼Œå¹¶å°†è¯·æ±‚è·¯ç”±åˆ°è¿™äº› Podã€‚ |
| **è´Ÿè½½å‡è¡¡** | å®¢æˆ·ç«¯éœ€è¦å°†æµé‡å‡åŒ€åœ°åˆ†æ•£åˆ°æ‰€æœ‰å¥åº·çš„åç«¯ Pod ä¸Šã€‚ |
| **Service å†…ç½®è´Ÿè½½å‡è¡¡æœºåˆ¶**ï¼ˆé€šå¸¸æ˜¯ Round-Robin æˆ– Least Connectionsï¼‰ï¼Œç¡®ä¿æµé‡å‡åŒ€åˆ†é…ã€‚ |



---

## ğŸ§ ä¸ºä»€ä¹ˆå«å®ƒ Serviceï¼ˆæœåŠ¡ï¼‰ï¼Ÿ

è™½ç„¶åå­—å¾ˆé€šç”¨ï¼Œä½† Kubernetes æ²¿ç”¨è¿™ä¸ªåç§°æ˜¯å› ä¸ºå®ƒç¡®å®å°†ä¸€ç»„ Pod æš´éœ²ä¸ºä¸€ä¸ªå¯ç”¨çš„â€œæœåŠ¡â€ï¼š

* **å¯¹å†…ï¼š** å®ƒä½¿é›†ç¾¤å†…çš„å…¶ä»– Pod å¯ä»¥é€šè¿‡ä¸€ä¸ªå›ºå®šçš„å†…éƒ¨ DNS åç§°è®¿é—®åç«¯åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œç›´æ¥è®¿é—® `my-database-service`ï¼‰ã€‚
* **å¯¹å¤–ï¼š** å®ƒå®šä¹‰äº†åº”ç”¨å¦‚ä½•ä»é›†ç¾¤å¤–éƒ¨ï¼ˆInternetï¼‰è¢«è®¿é—®ã€‚

æ‰€ä»¥ï¼Œä¸å…¶è¯´å®ƒæ˜¯ä¸€ä¸ªæœåŠ¡ï¼Œä¸å¦‚è¯´å®ƒæ˜¯ä¸€ä¸ª **â€œæœåŠ¡æŠ½è±¡/å‘ç°å±‚â€** æˆ– **â€œç½‘ç»œä»£ç†â€**ã€‚

---

## ğŸŒ K8s Service çš„ç±»å‹

Kubernetes Service æœ‰å‡ ç§ä¸åŒçš„ç±»å‹ï¼Œå†³å®šäº†å®ƒå¦‚ä½•å°†åº”ç”¨æš´éœ²ç»™ç½‘ç»œï¼š

| Service ç±»å‹ | ä½œç”¨ | æš´éœ²èŒƒå›´ |
| :--- | :--- | :--- |
| **ClusterIP (é»˜è®¤)** | ä¸º Service æä¾›ä¸€ä¸ªé›†ç¾¤å†…éƒ¨çš„ IP åœ°å€ï¼Œ**åªèƒ½**åœ¨é›†ç¾¤å†…éƒ¨è®¿é—®ã€‚ | ä»…é™é›†ç¾¤å†…éƒ¨ |
| **NodePort** | åœ¨é›†ç¾¤çš„æ¯ä¸ªèŠ‚ç‚¹ (Node) ä¸Šæ‰“å¼€ä¸€ä¸ªé™æ€ç«¯å£ï¼Œå¤–éƒ¨æµé‡å¯ä»¥é€šè¿‡ `<NodeIP>:<NodePort>` è®¿é—®ã€‚ | æš´éœ²ç»™å¤–éƒ¨ |
| **LoadBalancer** | ä»…åœ¨äº‘ç¯å¢ƒä¸­å¯ç”¨ã€‚å®ƒä¼šè‡ªåŠ¨åœ¨äº‘å¹³å° (å¦‚ AWS, GCP, Azure) ä¸Šé…ç½®ä¸€ä¸ªå¤–éƒ¨çš„äº‘è´Ÿè½½å‡è¡¡å™¨ï¼Œå¹¶å°†æµé‡å¯¼å…¥é›†ç¾¤ã€‚ | æš´éœ²ç»™å¤–éƒ¨ (æ¨è) |
| **ExternalName** | å°† Service æ˜ å°„åˆ°ä¸€ä¸ªå¤–éƒ¨çš„ DNS åç§°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªé›†ç¾¤å†…éƒ¨ IPã€‚ | å¤–éƒ¨ DNS |

æ€»è€Œè¨€ä¹‹ï¼Œæ‚¨å¯¹ `Service` åå­—çš„å›°æƒ‘æ˜¯åˆç†çš„ï¼Œä½†è¯·è®°ä½å®ƒåœ¨ K8s ä¸­çš„ç‰¹å®šå«ä¹‰ï¼š**ä¸€ä¸ªä¸ºä¸€ç»„åç«¯ Pod æä¾›ç¨³å®šç½‘ç»œè®¿é—®å’Œè´Ÿè½½å‡è¡¡çš„æŠ½è±¡å±‚ã€‚**

è¿™æ˜¯ä¸€ä¸ªç†è§£ Kubernetes æ ¸å¿ƒæœºåˆ¶çš„å…³é”®é—®é¢˜ã€‚**Label (æ ‡ç­¾)** å’Œ **Selector (é€‰æ‹©å™¨)** æ˜¯ Kubernetes ä¸­ç”¨äºç»„ç»‡ã€ç®¡ç†å’Œé€‰æ‹©èµ„æºçš„åŸºæœ¬æœºåˆ¶ã€‚å®ƒä»¬å…±åŒåä½œï¼Œå®ç°äº† Kubernetes å¼ºå¤§çš„è‡ªåŠ¨åŒ–å’Œè§£è€¦èƒ½åŠ›ã€‚

---

## ğŸ·ï¸ 1. Labelï¼ˆæ ‡ç­¾ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ

**Label** æ˜¯é™„åŠ åˆ° Kubernetes å¯¹è±¡ï¼ˆå¦‚ Podã€Nodeã€Serviceã€Deployment ç­‰ï¼‰ä¸Šçš„ **é”®å€¼å¯¹ (Key-Value pairs)**ã€‚

### ä½œç”¨ï¼šç»„ç»‡å’Œè¯†åˆ«

* **è¯†åˆ«å±æ€§ï¼š** Label ç”¨äºä»¥å¯¹ç”¨æˆ·æœ‰æ„ä¹‰çš„æ–¹å¼è¯†åˆ«å¯¹è±¡ã€‚
* **è§£è€¦ï¼š** å®ƒä»¬ä¸ç›´æ¥è€¦åˆåˆ°æ ¸å¿ƒç³»ç»Ÿï¼Œè€Œæ˜¯ç”¨äºç”¨æˆ·è‡ªå®šä¹‰çš„ç»„ç»‡ç»“æ„ã€‚
* **ç¤ºä¾‹ï¼š**
    * **åº”ç”¨ç±»å‹ï¼š** `app: frontend` æˆ– `app: database`
    * **ç¯å¢ƒï¼š** `env: production` æˆ– `env: development`
    * **ç‰ˆæœ¬ï¼š** `version: v1.2.3`

**ç‰¹ç‚¹ï¼š**

* ä¸€ä¸ªå¯¹è±¡å¯ä»¥æœ‰å¤šä¸ª Labelã€‚
* Label å¯ä»¥åœ¨å¯¹è±¡åˆ›å»ºåéšæ—¶æ·»åŠ æˆ–ä¿®æ”¹ã€‚

---

## ğŸ” 2. Selectorï¼ˆé€‰æ‹©å™¨ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ

**Selector** æ˜¯ä¸€ç§è¡¨è¾¾å¼ï¼Œç”¨äº **åŒ¹é…** å…·æœ‰ç‰¹å®š Label çš„ä¸€ç»„å¯¹è±¡ã€‚å®ƒæ˜¯ Kubernetes è‡ªåŠ¨åŒ–å’Œæ§åˆ¶å¾ªç¯å·¥ä½œçš„æ ¸å¿ƒæœºåˆ¶ã€‚

### ä½œç”¨ï¼šå®šä½å’Œç®¡ç†

Selector çš„ä½œç”¨æ˜¯å‘Šè¯‰æ§åˆ¶å™¨ï¼ˆControllerï¼‰æˆ– Service åº”è¯¥ç®¡ç†æˆ–è·¯ç”±åˆ°å“ªäº›å¯¹è±¡ã€‚

### å¸¸è§çš„ Selector ä½¿ç”¨åœºæ™¯ï¼š

1.  **ReplicaSet/Deployment Selector:** ç”¨äºå‘Šè¯‰æ§åˆ¶å™¨åº”è¯¥ç®¡ç†å“ªäº› Podï¼ˆç¡®ä¿å®ƒä»¬æ•°é‡æ­£ç¡®ï¼‰ã€‚
2.  **Service Selector:** ç”¨äºå‘Šè¯‰ Service åº”è¯¥å°†å®¢æˆ·ç«¯è¯·æ±‚è·¯ç”±åˆ°å“ªäº›åç«¯çš„ Podã€‚
3.  **Node Selector:** ç”¨äºé™åˆ¶ Pod åªèƒ½åœ¨å…·æœ‰ç‰¹å®š Label çš„ Node ä¸Šè°ƒåº¦ã€‚

**ç¤ºä¾‹ï¼š**

ä¸€ä¸ª Service çš„ Selector å¯èƒ½æ˜¯ `app: frontend`ï¼Œè¿™æ„å‘³ç€å®ƒåªä¼šå°†æµé‡è·¯ç”±åˆ°æ‰€æœ‰å¸¦æœ‰ `app: frontend` æ ‡ç­¾çš„ Podã€‚

---

## ğŸ”— 3. Label å’Œ Selector çš„å…³ç³»ï¼ˆæ ¸å¿ƒï¼‰

**Label å’Œ Selector ä¹‹é—´çš„å…³ç³»æ˜¯â€œæ ‡è¯†ä¸é€‰æ‹©â€çš„å…³ç³»ã€‚**

* **Label è´Ÿè´£æ ‡è¯† (Identity):** å®ƒä»¬æ˜¯å¯¹è±¡èº«ä¸Šçš„â€œèº«ä»½æ ‡ç­¾â€ã€‚
* **Selector è´Ÿè´£é€‰æ‹© (Selection):** å®ƒä»¬æ˜¯â€œæŸ¥æ‰¾æ¡ä»¶â€ï¼Œæ ¹æ® Label å»æ‰¾å¯¹åº”çš„å¯¹è±¡ã€‚

### å…³ç³»å›¾ç¤º

| èµ„æº | å®šä¹‰ Selector | å®šä¹‰ Label | æ ¸å¿ƒåŠŸèƒ½ |
| :--- | :--- | :--- | :--- |
| **Deployment / ReplicaSet** | `spec.selector.matchLabels` | `spec.template.metadata.labels` | **ç®¡ç† Podï¼š** ReplicaSet ä½¿ç”¨ Selector æ‰¾åˆ°ç”±å®ƒç®¡ç†çš„ Podï¼Œå¹¶ç¡®ä¿æ•°é‡ç­‰äº `replicas`ã€‚ |
| **Service** | `spec.selector` | ä½œç”¨äº Pod | **æµé‡è·¯ç”±ï¼š** Service ä½¿ç”¨ Selector æ‰¾åˆ°åç«¯å¥åº·çš„ Pod åˆ—è¡¨ï¼Œå¹¶å°†æµé‡è´Ÿè½½å‡è¡¡åˆ°è¿™äº› Podã€‚ |

**å…³é”®çš„åŒ¹é…åŸåˆ™ï¼š**

1.  **æ§åˆ¶å™¨å’Œ Pod æ¨¡æ¿ï¼š** åœ¨å®šä¹‰åƒ `Deployment` è¿™æ ·çš„æ§åˆ¶å™¨æ—¶ï¼Œæ‚¨å¿…é¡»ç¡®ä¿ `spec.selector.matchLabels` ä¸­å®šä¹‰çš„æ ‡ç­¾é€‰æ‹©å™¨ï¼Œä¸ `spec.template.metadata.labels` ä¸­å®šä¹‰çš„ Pod æ ‡ç­¾**å®Œå…¨ä¸€è‡´**ã€‚å¦‚æœå®ƒä»¬ä¸åŒ¹é…ï¼Œæ§åˆ¶å™¨å°†æ— æ³•æ‰¾åˆ°æˆ–åˆ›å»ºè‡ªå·±çš„ Podã€‚
2.  **Pod å’Œ Serviceï¼š** Service ä½¿ç”¨å…¶ Selector æ¥æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ Podï¼Œè¿™äº› Pod æˆä¸º Service çš„**ç«¯ç‚¹ (Endpoints)**ã€‚

