# SRE Basic Terms

- [SRE Basic Terms](#sre-basic-terms)
  - [REFERENCE](#reference)
  - [NOTE](#note)
  - [Distributed System](#distributed-system)
  - [Containerization](#containerization)
  - [Containers](#containers)
  - [Docker](#docker)
  - [Cloud Native Apps](#cloud-native-apps)
  - [Cluster](#cluster)
  - [Infrastructure as Code (IaC)](#infrastructure-as-code-iac)
  - [Infrastructure as a Service (IaaS)](#infrastructure-as-a-service-iaas)
  - [Kubernetes](#kubernetes)
    - [Kubnernetes API](#kubnernetes-api)
    - [Building Blocks](#building-blocks)
  - [Pod](#pod)
  - [Nodes](#nodes)
  - [Helm](#helm)
  - [Terraform](#terraform)
  - [Terragrunt](#terragrunt)
  - [Multitenancy](#multitenancy)
  - [YAML](#yaml)
  - [K8S Service çš„çœŸå®å«ä¹‰å’Œä½œç”¨](#k8s-service-çš„çœŸå®å«ä¹‰å’Œä½œç”¨)
    - [æ ¸å¿ƒç—›ç‚¹ä¸ Service çš„è§£å†³æ–¹æ¡ˆ](#æ ¸å¿ƒç—›ç‚¹ä¸-service-çš„è§£å†³æ–¹æ¡ˆ)
    - [ä¸ºä»€ä¹ˆå«å®ƒ Serviceï¼ˆæœåŠ¡ï¼‰ï¼Ÿ](#ä¸ºä»€ä¹ˆå«å®ƒ-serviceæœåŠ¡)
    - [K8s Service çš„ç±»å‹](#k8s-service-çš„ç±»å‹)
    - [1. Labelï¼ˆæ ‡ç­¾ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ](#1-labelæ ‡ç­¾æ˜¯ä»€ä¹ˆ)
      - [ä½œç”¨ï¼šç»„ç»‡å’Œè¯†åˆ«](#ä½œç”¨ç»„ç»‡å’Œè¯†åˆ«)
    - [2. Selectorï¼ˆé€‰æ‹©å™¨ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ](#2-selectoré€‰æ‹©å™¨æ˜¯ä»€ä¹ˆ)
      - [ä½œç”¨ï¼šå®šä½å’Œç®¡ç†](#ä½œç”¨å®šä½å’Œç®¡ç†)
      - [å¸¸è§çš„ Selector ä½¿ç”¨åœºæ™¯](#å¸¸è§çš„-selector-ä½¿ç”¨åœºæ™¯)
    - [3. Label å’Œ Selector çš„å…³ç³»ï¼ˆæ ¸å¿ƒï¼‰](#3-label-å’Œ-selector-çš„å…³ç³»æ ¸å¿ƒ)
      - [å…³ç³»å›¾ç¤º](#å…³ç³»å›¾ç¤º)
  - [äº‘åŸç”Ÿ (Cloud Native)](#äº‘åŸç”Ÿ-cloud-native)
    - [äº‘åŸç”Ÿçš„æ ¸å¿ƒä¸‰è¦ç´ ](#äº‘åŸç”Ÿçš„æ ¸å¿ƒä¸‰è¦ç´ )
      - [1. æŠ€æœ¯æ¶æ„ (Architecture)](#1-æŠ€æœ¯æ¶æ„-architecture)
      - [2. è‡ªåŠ¨åŒ–å’Œç®¡ç† (Automation \& Management)](#2-è‡ªåŠ¨åŒ–å’Œç®¡ç†-automation--management)
      - [3. æ–‡åŒ–å’Œæµç¨‹ (Culture \& Process)](#3-æ–‡åŒ–å’Œæµç¨‹-culture--process)
    - [äº‘åŸç”Ÿçš„æœ€ç»ˆç›®æ ‡](#äº‘åŸç”Ÿçš„æœ€ç»ˆç›®æ ‡)
  - [Service Meshï¼ˆæœåŠ¡ç½‘æ ¼ï¼‰ä»‹ç»](#service-meshæœåŠ¡ç½‘æ ¼ä»‹ç»)
    - [æ ¸å¿ƒç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ](#æ ¸å¿ƒç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ)
    - [Service Mesh çš„æ¶æ„](#service-mesh-çš„æ¶æ„)
      - [1. æ•°æ®å¹³é¢ (Data Plane)](#1-æ•°æ®å¹³é¢-data-plane)
      - [2. æ§åˆ¶å¹³é¢ (Control Plane)](#2-æ§åˆ¶å¹³é¢-control-plane)
    - [Service Mesh çš„ä¸»è¦ä¼˜åŠ¿](#service-mesh-çš„ä¸»è¦ä¼˜åŠ¿)
  - [Istio](#istio)
    - [Istio çš„æ ¸å¿ƒå®šä½](#istio-çš„æ ¸å¿ƒå®šä½)
      - [Istio çš„ä¸‰å¤§ç›®æ ‡](#istio-çš„ä¸‰å¤§ç›®æ ‡)
    - [Istio çš„æ¶æ„ç»„æˆ](#istio-çš„æ¶æ„ç»„æˆ)
      - [1. æ•°æ®å¹³é¢ (Data Plane)](#1-æ•°æ®å¹³é¢-data-plane-1)
    - [Istio çš„å…³é”®åŠŸèƒ½è¯¦è§£](#istio-çš„å…³é”®åŠŸèƒ½è¯¦è§£)
      - [1. æµé‡ç®¡ç† (Traffic Management)](#1-æµé‡ç®¡ç†-traffic-management)
      - [2. å®‰å…¨ (Security)](#2-å®‰å…¨-security)
      - [3. å¯è§‚æµ‹æ€§ (Observability)](#3-å¯è§‚æµ‹æ€§-observability)
    - [Istio çš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜](#istio-çš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜)
  - [Kubernetes/äº‘åŸç”Ÿåˆ†å±‚æ¨¡å‹](#kubernetesäº‘åŸç”Ÿåˆ†å±‚æ¨¡å‹)
    - [Kubernetes/äº‘åŸç”Ÿåˆ†å±‚æ¨¡å‹ (å®è·µçº¦å®š)](#kubernetesäº‘åŸç”Ÿåˆ†å±‚æ¨¡å‹-å®è·µçº¦å®š)
      - [L1ï¼šåŸºç¡€è®¾æ–½å±‚ (The Infrastructure Layer)](#l1åŸºç¡€è®¾æ–½å±‚-the-infrastructure-layer)
      - [L2ï¼šKubernetes æ§åˆ¶å±‚/å¹³å°å±‚ (The Control Plane \& Platform Layer)](#l2kubernetes-æ§åˆ¶å±‚å¹³å°å±‚-the-control-plane--platform-layer)
      - [L3ï¼šåº”ç”¨æœåŠ¡å±‚ (The Application Services Layer)](#l3åº”ç”¨æœåŠ¡å±‚-the-application-services-layer)
      - [L4ï¼šç”¨æˆ·ä½“éªŒå±‚ (The End-User Experience Layer)](#l4ç”¨æˆ·ä½“éªŒå±‚-the-end-user-experience-layer)
    - [ğŸ“œ å®˜æ–¹æ–‡æ¡£è¯´æ˜](#-å®˜æ–¹æ–‡æ¡£è¯´æ˜)

## REFERENCE

[Cloud Native Glossary](https://glossary.cncf.io/)
[What is SRE?](https://www.redhat.com/en/topics/devops/what-is-sre)

## NOTE

- Turn to Helm for packaging applications, Terraform for infrastructure provisioning, and Terragrunt for orchestrating Terraform configurations.
- The combination of Docker, Helm, Terraform, and Terragrunt empowers teams to deploy complex applications to Kubernetes in a streamlined, declarative, and consistent manner.
  - Docker images encapsulate the application code and its dependencies, making deployments portable and reliable. 
  - Helm charts simplify Kubernetes deployments by providing reusable and versioned templates. 
  - Terraform offers a declarative approach to provisioning Kubernetes infrastructure. 
  - Finally, Terragrunt orchestrates the entire process, ensuring consistency and DRY configurations across environments.
- Ideas???
  - Be lazy; automatic everything to avoid toils and to be consistent
  - introduce another layer of abstraction; use these declarative languages; not worry about how these actually do
  - having a single source of truth helps visibility, maintainability and increases durability and/or stability
  - having control over what is going to be deployed is a must-have
- Minikube
  - [Minikub](https://kubernetes.io/docs/tasks/tools/), which offers an excellent way to work with a single-node Kubernetes cluster locally.
- Helm
  - [Helm installation](https://helm.sh/docs/v2/using_helm/#installing-helm)
- Terraform vs Kubernetes
  - Terraform is a tool for provisioning and managing infrastructure as code, such as servers, databases, and networking across multiple cloud providers.
  - Kubernetes is a container orchestration platform that manages the deployment, scaling, and operation of containerized applications.
  - Terraform builds the infrastructure where Kubernetes might run. Kubernetes then manages application workloads on that infrastructure. They often work together but solve different layers of the stack, with Terraform setting up infrastructure and Kubernetes managing application-level operations.
  - Terraform focuses on provisioning infrastructure components and targets the Infrastructure as Code space.
  - Kubernetes aims to enable us to run container workloads and targets the container orchestration space.
  - Kubernetes and Terraform complement each other since they operate at two different levels and can be utilized in parallel.
  - A typical model that cloud practitioners adopt is to use Terraform to provision infrastructure resources (e.g. Kubernetes clusters) and use Kubernetes to manage the containerized apps that run on top of the clusters.
  - Another approach is to use Terraform to manage Kubernetes-specific application components as well. This model has the advantage of adding the Terraform workflow to Kubernetes components. This way, IT operators can detect configuration drift on Kubernetes and manage infrastructure and application resources with the same workflow and configuration language. This approach has a significant disadvantage since Terraform requires a well-defined schema for each managed resource. Thus each Kubernetes resource needs to be translated into a Terraform schema to be available. This dependency makes maintaining Kubernetes resources through Terraform cumbersome at times.
- Terraform vs Helm
  - Infrastructure Provisioning vs. Application Deployment;
    - Terraform excels in provisioning and managing infrastructure resources across various cloud providers. It is suitable for defining and managing virtual machines, Kubernetes clusters, storage, and networking components.
    - Helm, however, focuses on packaging and deploying applications on Kubernetes. It streamlines the installation, upgrading, and rollback of containerized applications.
  - Infrastructure as Code vs. Application Packaging
    - Terraform's strength lies in its ability to define infrastructure resources as code. This approach ensures that infrastructure changes can be version-controlled, reviewed, and shared among teams.
    - Helm, being a package manager, specializes in bundling applications and their configurations into portable packages. This allows for consistent deployment across different environments.
- jife

## Distributed System

A distributed system is a collection of autonomous computing elements connected over a network that appears to users as a single coherent system. Generally referred to as nodes, these components can be hardware devices (e.g. computers, mobile phones) or software processes. Nodes are programmed to achieve a common goal and, to collaborate, they exchange messages over the network.

Distributed systems allow for horizontal scaling (e.g. adding more nodes to the system whenever needed). This can be automated allowing a system to handle a sudden increase in workload or resource consumption.

## Containerization

Containerization is the process of packaging of application code including libraries and dependencies required to run the code into a single lightweight executableâ€”called container image.

Container images are lightweight (unlike traditional VMs) and the containerization process requires a file with a list of dependencies. A container image is stored by a unique identifier that is tied to its exact content and configuration. As containers are scheduled and rescheduled, they are always reset to their initial state which eliminates configuration drift.

a next-generation virtualization and automation solution after Hypervisor Virtualization. It widely applied today because of the breakthrough effect with outstanding advantages in creating and deploying applications faster, more scalable and more securely than traditional methods.

## Containers

A container is a running process with resource and capability constraints managed by a computerâ€™s operating system. The files available to the container process are packaged as a container image. Containers run adjacent to each other on the same machine, but typically the operating system prevents the separate container processes from interacting with each other.

## Docker

a number one software to create, manage and run containers

Encapsulating Application Code for Portability. Docker has become the de facto standard for containerization.

Docker containers have revolutionized software development by encapsulating applications, libraries, and dependencies in a consistent and portable environment. With Docker, you can package your project's application code and all its dependencies into a single, lightweight image. This image can be easily shared, version-controlled, and deployed across different environments without worrying about variations in the underlying infrastructure.

By leveraging Docker images, we can reduce the infamous "it works on my machine" problem. This consistency is especially valuable when deploying applications to Kubernetes clusters, as it eliminates the need to deal with subtle environment differences that could lead to production issues.

Docker is the platform for creating, deploying, and running applications in containers. It provides a consistent environment for applications using containers. It can be used as the container runtime for Kubernetes. 

## Cloud Native Apps

Cloud native applications are specifically designed to take advantage of innovations in cloud computing. These applications integrate easily with their respective cloud architectures, taking advantage of the cloudâ€™s resources and scaling capabilities. It also refers to applications that take advantage of innovations in infrastructure driven by cloud computing. Cloud native applications today include apps that run in a cloud providerâ€™s datacenter and on cloud native platforms on-premise.

## Cluster

A cluster is a group of computers or applications called nodes that work together towards a common goal. In cloud native computing, the term is often applied when talking about a Kubernetes cluster. It can be seen as a specific kind of distributed system where the nodes are a bit more tightly coupled.

## Infrastructure as Code (IaC)

Infrastructure as code is the practice of storing the definition of infrastructure as one or more files. This replaces the traditional model where infrastructure as a service is provisioned manually, usually through shell scripts or other configuration tools

Building applications in a cloud native way requires infrastructure to be disposable and reproducible.

By representing the data center resources such as servers, load balancers, and subnets as code, it allows infrastructure teams to have a single source of truth for all configurations and also allows them to manage their data center in a CI/CD pipeline, implementing version control and deployment strategies.

## Infrastructure as a Service (IaaS)

Infrastructure
Infrastructure as a service, or IaaS, is a cloud computing service model that offers physical or virtualized compute, storage, and network resources on-demand on a pay-as-you-go model. Cloud providers own and operate the hardware and software, available to consumers in public, private, or hybrid cloud deployments.

An on-demand infrastructure allows them to rent compute resources as needed and defer large capital expenditures, or CAPEX, while giving them the flexibility to scale up or down.

IaaS reduces the upfront costs of experimenting or trying a new application and provides facilities to rapidly deploy infrastructure. A cloud provider is an excellent option for development or test environments, which helps developers experiment and innovate.

## Kubernetes

Kubernetes, often abbreviated as K8s, is an open source container orchestrator. It automates the lifecycle of containerized applications on modern infrastructures, functioning as a â€œdatacenter operating systemâ€ that manages applications across a distributed system.

Kubernetes schedules containers across nodes in a cluster, bundling several infrastructure resources such as load balancer, persistent storage, etc. to run containerized applications.

Kubernetes enables automation and extensibility, allowing users to deploy applications declaratively (see below) in a reproducible way.

Similar to traditional infrastructure as code tools, Kubernetes helps with automation but has the advantage of working with containers. Containers are more resistant to configuration drift than virtual or physical machines.

Additionally, Kubernetes works declaratively, which means that instead of operators instructing the machine how to do something, they describe â€” usually as manifest files (e.g., YAML) â€” what the infrastructure should look like. Kubernetes then takes care of the â€œhowâ€. This results in Kubernetes being extremely compatible with infrastructure as code.

Kubernetes (K8s) is an open-source system for container orchestration, automating deployments, and managing containerized apps. Its powerful orchestration system enables applications to scale seamlessly and achieve high availability.

Kubernetes strives to be cloud agnostic at its core, providing great flexibility in running workloads across cloud and on-premises environments.

One of its main benefits is the self-healing capabilities it provides. Containers that fail are automatically restarted and rescheduled, nodes can be configured to be automatically replaced, and traffic is served only by healthy components based on health checks.  

Rollouts are handled progressively, and Kubernetes provides smart mechanisms to monitor application health during deployments. Rolling back problematic changes happens automatically if the application health doesnâ€™t report a healthy status after a new deployment.

Kubernetes handles service discovery and load-balances traffic between similar pods natively without the need for complex external solutions.

Kubernetes provides a cluster of nodes, a group of worker machines that run containerized applications. Each node hosts pods that hold application workload containers. The brain of the whole system is the control plane. Each cluster consists of several components that manage the worker nodes and pods and guarantee operational continuity.

A **pod** is the smallest deployable unit in Kubernetes, encapsulating one or more containers that share storage and network resources. This flexibility is essential for implementing complex applications effectively within a Kubernetes cluster.

Scaling an application in Kubernetes typically involves increasing the number of Pods that run the application, ensuring better resource utilization and improved performance.

[Cluster Architecture](https://kubernetes.io/docs/concepts/architecture/) and [Core Components](https://kubernetes.io/docs/concepts/overview/components/): 

- Control Plane
  - The **API server** is the component exposing the Kubernetes API and operates as the front-end of the control plane by handling all the communication between other parts.
  - The **etcd** (a distributed /etc) component is used to store all cluster data and state. It is a distributed reliable key-value store used by kubernetes to store all configuration and metadata used to manage the cluster.
  - The **scheduler** manages how pods are assigned to nodes and takes all the workload scheduling decisions. It is responsible for distributing work among multiple nodes by deciding which nodes will run specific pods. This function is crucial for optimizing resource utilization and ensuring efficient operation within a Kubernetes cluster. 
  - The **controller manager** components run different controller processes to ensure that the clusterâ€™s desired state matches its current state.
  - The **cloud controller** manager integrates Kubernetes clusters with external cloud providers, embeds their logic, and links the Kubernetes API with the Cloud Providerâ€™s API.
- On each worker node
  - the **kubelet** (the primary "node agent") is the agent responsible for running containers in pods,
  - **kube-proxy** is the component that adds the necessary networking capabilities for communication between pods and nodes.

- kubectl
  - kubectl run; start a pod; deploy an application
  - kubectl cluster-info
  - kubectl get nodes; list all nodes of a cluster; find out how many nodes in this cluster
    - kubectl get nodes - o wide; // get os-image and kernel-version
  - kubectl config view; // show the current Config
  - kubectl get pods --namespace kube-system; // see Helm's Tiller running
  - kubectl describe pod (nginx); // See a pod detail
  - kubectl describe deploy tiller-deploy --namespace=kube-system
  - kubectl apply -f pod.yaml // create(apply) the pod
  - kubectl version // the version of Kubernetes

 **container runtime** is the underlying framework responsible for running applications within containers, such as those created with Docker.
 **kubectl** is the command-line tool specifically designed for managing Kubernetes clusters, allowing you to interact with cluster resources and deploy applications effectively.

### Kubnernetes API

API Server:

- The central component that exposes the Kubernetes API.
- All communication with the cluster, whether from kubectl, other control plane components, or external tools, goes through the API server.

API Objects:

- Everything in Kubernetes is represented as an API object (e.g., Pods, Deployments, Services, ConfigMaps, Secrets, Nodes, Namespaces).
- These objects have a defined schema and state, which is stored in etcd.

API Verbs:

- Standard operations that can be performed on API objects:
- get: Retrieve a specific object.
- list: Retrieve a collection of objects (e.g., all Pods in a Namespace).
- create: Create a new object.
- update: Replace an existing object with a new definition.
- patch: Apply partial updates to an existing object.
- delete: Delete an object.
- watch: Monitor changes to objects in real-time.

API Groups and Versions:

- Kubernetes API objects are organized into API Groups (e.g., apps, batch, networking.k8s.io).
- Each API Group can have multiple versions (e.g., v1, v1beta1). This allows for API evolution and backward compatibility.

Common Interactions:

- Inspecting Resources: kubectl get <resource>, kubectl describe <resource> <name>
- Creating/Updating Resources: kubectl apply -f <manifest.yaml>
- Deleting Resources: kubectl delete <resource> <name>
- Managing Deployments: kubectl rollout status deployment/<name>, kubectl scale deployment/<name> --replicas=<count>
- Accessing Logs: kubectl logs <pod_name>
- Debugging Pods: kubectl exec -it <pod_name> -- <command>, kubectl port-forward pod/<pod_name> <local_port>:<remote_port>

Note: While kubectl provides a convenient command-line interface to interact with the Kubernetes API, direct API interaction can be performed using client libraries in various programming languages (Go, Python, Java, etc.) or by making direct HTTP requests.

### Building Blocks

- Deployments
  - tell Kubernetes how you want your application's lifecycle handled, including scaling and updates.
  - Rolling Updatesï¼šfacilitate smooth, zero-downtime application updates. Gradually replaces the old Pods with new ones at a controlled rate, ensuring continuous availability.
  - Rollbacks: If a new deployment turns out to be unstable, can easily roll back to a previous, stable version.
  - Self-Healing: automatically restarting or replacing failed Pods to maintain the desired application state.
  - Stateless Focus: ideal for stateless applications, where individual Pod instances are interchangeable and do not require persistent data across restarts.
- Services
  - provides a stable, persistent network endpoint for a set of Pods. Because Pods are ephemeral (they come and go, and their IP addresses change), you need a stable way to communicate with them. A Service is that abstraction.
  - Services assign a stable IP address and DNS name to a group of Pods (determined by a selector), and load-balance traffic across them.
  - ClusterIP: Exposes the Service on an internal cluster IP. This makes the Service only reachable from within the cluster, which is ideal for internal microservice communication.
  - NodePort: Exposes the Service on a static port on each Node's IP. This makes the Service accessible from outside the cluster via any Node's IP and the static port.
  - LoadBalancer: Exposes the Service externally using a cloud provider's load balancer, which routes traffic to your Pods.
  - ExternalName: Maps a Service to an arbitrary external DNS name, not a Pod selector.
  - Headless: A special type that doesn't assign a cluster IP, but rather provides stable DNS entries for each individual Pod, which is essential for StatefulSets.
  - Analogy: A Service is like a telephone number for a department in a large company. You call one consistent number, and the call is routed to any available employee (Pod)
- ConfigMaps
  - store non-confidential configuration data as key-value pairs or as entire configuration files. They decouple configuration details from your application code. manage environment-specific settings (like feature flags, logging levels, or database URLs) without having to rebuild your container images.
  - Decoupling: Promotes best practices by separating configuration from the application image, making the container images more portable and reusable.
  - Consumption Methods: Pods can consume ConfigMap data as environment variables, command-line arguments, or as files mounted in a volume inside the container.
  - Dynamic Updates: When mounted as volumes, some applications can even access updated configuration values without needing a restart.
  - Analogy: A ConfigMap is like a general settings file cabinet for the factory.
- Secrets
  - similar to ConfigMaps but are specifically designed for storing and managing sensitive information, such as passwords, API tokens, and SSH keys.  provide a safer and more flexible way to use confidential data than hardcoding it in a Pod definition or container image.
  - Security Focus: Data in Secrets is base64 encoded by default (to prevent accidental display, not true encryption, though they can be encrypted at rest on the cluster).
  - Access Control: Access to Secrets can be restricted using Role-Based Access Control (RBAC) within Kubernetes.
  - Consumption Methods: Like ConfigMaps, Secrets can be exposed to Pods as environment variables or as files in a mounted volume. Using volumes is generally preferred for better security and dynamic updates.
  - Analogy: A Secret is like a secure safe in the factory manager's office.
- Ingress
  - manages external access to the Services within a cluster, typically HTTP traffic. It provides advanced traffic routing capabilities beyond what a Service alone can offer. acts as the entry point for traffic coming into your cluster from the outside world. It can provide load balancing, SSL/TLS termination, and name-based virtual hosting.
  - External Access Routing: It defines rules for how traffic should be routed to different Services based on the request URL path or hostname.
  - Centralized Traffic Management: Instead of creating multiple LoadBalancer Services (which can be costly), a single Ingress controller can manage access to many services, saving costs.
  - TLS/SSL: It can handle the termination of secure connections (HTTPS), managing the necessary certificates stored in Secrets.
  - Analogy: Ingress is the main reception desk and directory of a large office building. All external visitors (external traffic) arrive here.
- Summary
  - Deployments manage your application instances; Services provide reliable internal network connectivity; ConfigMaps and Secrets handle configuration and sensitive data management securely; and Ingress opens up controlled, intelligent access from the outside world. 
- etc

## Pod

Within a Kubernetes environment, a pod acts as the most basic deployable unit. It represents an essential building block for deploying and managing containerized applications. Each pod contains a single application instance and can hold one or more containers. Kubernetes manages pods as part of a larger deployment and can scale pods vertically or horizontally as needed.

While containers generally act as independent units that run and control a particular workload, there are cases when containers need to interact and be controlled in a tightly coupled manner.

Pods bundle closely tied containers into a single unit, significantly simplifying container operations. 

Memory and CPU allocation can be defined either on a pod level, allowing the containers inside to share resources in a flexible way, or per container.

## Nodes

A node is a computer that works in concert with other computers, or nodes, to accomplish a common task. Take your laptop, modem, and printer, for example. They are all connected over your wifi network communicating and collaborating, each representing one node. In cloud computing, a node can be a physical computer, a virtual computer, referred to as a VM, or even a container

## Helm

Helm is a package manager for Kubernetes that *simplifies the deployment and management of applications on Kubernetes clusters*.  It solves resource issues by letting you bundle resource files into a single package. They are defined in Yaml files, which are often called manifests. Helm uses a packaging format called charts. A **Helm chart** is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.

Helm, essentially a package manager for Kubernetes, simplifies the deployment process by providing a templating engine and a collection of pre-configured packages called "charts." These charts define the resources needed to run an application in Kubernetes, including deployments, services, and configuration files.

Helm simplifies the deployment and management of applications on K8S by abstracting away the complexities of **configuring and managing** individual K8S resources.

With Helm charts, developers can easily define, version, and manage complex Kubernetes deployments as code.The templating engine allows for parameterization, enabling the reuse of chart configurations with different values for each environment (e.g., development, staging, production). This abstraction helps standardize the deployment process and reduces the chances of human errors when deploying complex applications.

Key Features of Helm:

- Templates: Helm charts are templates for Kubernetes YAML files, which can be customized with variables and flow control statements.
- Package Management: Helm manages Kubernetes applications through Helm charts which support versioning, which makes it easy to roll back to an earlier version if something goes wrong.
- Dependencies: Helm charts can define dependencies on other charts, allowing complex applications to be composed from individual components.

Helm has 4 basic concepts:

- Chart: a collection of YAML files; bundle of the Kubernetes resources needed to build a Kubernetes application. For ease of visualization, Helm Chart can be compared like a Docker Image.
- Config: a configuration in the values.yaml file, which contains configuration explicit to a release of Kubernetes application.
- Release: a chart instance is loaded into Kubernetes. It can be viewed as a version of the Kubernetes application running based on Chart and associated with a specific Config.
- Repositories: a repository of published Charts. It provides a centralized location for sharing Helm charts. These can be private repositories that are only used within the company or public through the Helm Hub.

Helm has a fairly simple client-server architecture, including a CLI client and an in-cluster server running in the Kubernetes cluster

- **Helm Client**: Provides the developer to use it a command-line interface (CLI) to work with Charts, Config, Release, Repositories. Helm Client will interact with Tiller Server, to perform various actions such as install, upgrade and rollback with Charts, Release.
- **Tiller** Server (in Helm 2, but not in Helm 3): an in-cluster server in the Kubernetes cluster, interacting with the Helm Client and communicating with the Kubernetes API server. Thus, Helm can easily manage Kubernetes with tasks such as install, upgrade, query and remove for Kubernetes resources. As Role-Based Access Control (RBAC) gained traction in Kubernetes, users gained the capability to manage granular and precise permissions for Kubernetes resources and actions. In Helm 3, the middle component â€œTillerâ€ was completely removed. Now the security is being handled by the RBAC. In Helm 3, the client can directly communicate with the API server of your Kubernetes cluster.

Helm focuses on application packaging and deployment. Helm is a Kubernetes package manager that streamlines the installation and management of containerized applications. It uses "charts" as packaged applications, containing all the necessary Kubernetes resources, configuration files, and dependencies. This allows for simplified application deployment, version management, and rollbacks.

A Helm Chart is **a packaging format** used to define, install, upgrade, deploy, share, and manage Kubernetes applications. It lets you group multiple Kubernetes resourcesâ€”like Deployments, Services, and Persistent Volumesâ€”into one reusable unit. Helm Charts automate complex Kubernetes deployments by templating configurations and managing lifecycle upgrades. Charts describe even the most complex apps, provide repeatable application installation, and serve as a single point of authority.

Deploying real-world applications in Kubernetes usually involves several moving parts: app containers, config maps, secrets, service accounts, and storage. Managing all of this manually becomes inefficient fast. Helm solves this by turning infrastructure into modular, repeatable packages. It reduces human error, accelerates CI/CD pipelines, and makes rollbacks as simple as a single command.

A Helm chart describes how to manage a specific application on Kubernetes. It consists of metadata that describes the application, plus the infrastructure needed to operate it in terms of the standard Kubernetes primitives. Each chart references one or more container images that contain the application code to be run.

Despite the fact we can run application containers using the Kubernetes command line (kubectl), the easiest way to run workloads in Kubernetes is using the ready-made Helm charts.

How a Helm Chart Is Organized

- Each Helm Chart is a folder that follows a clear structure, making it easier to manage, update, and reuse across environments.
- **Chart.yaml** â€“ It contains metadata info like the chart name, version, and a brief description.
- **values.yaml** â€“ Defines default configuration values that can be overridden at deployment. the template files collect deployment information from this file. It can customize the helm chart.
- **templates/** â€“ Holds the actual Kubernetes manifest templates using Go templating. stores the actual yaml files. It holds all the configurations for the application. It also includes a tests directory. It use [Golang](https://www.geeksforgeeks.org/go-language/go/) templating format to assemble the configurations from values.yaml or command line and convert the complete configuration into a Kubernetes manifest.
- **charts/** â€“ (Optional) Includes subcharts or external dependencies that the main chart relies on.

Helm terminologies and core components

- Chart: It is a package of containing pre-configured kubernetes resources.
- Release: It is an instance of running charts in a kubernetes resource.
- Repository: It is a collection of charts that can be shared and stored.
- Values: It is useful for configuration settings that can help in customizing chart deployments.
- Helm Chart: It is a helm package manager that contains a collection of kubernetes resource definitions.

Helm is built for scaling, upgrading, and maintaining production environments.

good habits:

- Keep values.yaml files environment-specific (dev, staging, prod)
- Store your charts in version control alongside app code
- Donâ€™t hardcode secretsâ€”use tools like Sealed Secrets or a vault
- Lint charts before deploying with **helm lint**
- Use subcharts for reusable services across apps

commands:

- helm lint: correct any syntax error
- helm template:  replace the placeholder object with original values and show the complete file in console.
- helm install --dry-run: pretend to install the manifests file and try to display the manifests files.
- helm install prod: // kubectl create namespace prod (dev) in advance
- kubectl get all -n prod: confirm the deployment is successful
- helm list: check the number of releases in the host; list all Helm releases.
- helm upgrade (prod)ï¼šif any environment needs to update with image details, versions, replica count, storage memory etcâ€¦, we can do that with upgrade by without bringing down the original state; update a Helm release.
- helm rollback (prod): rollback to previous state
- helm package: package the chart and deploy it to Github, S3, or any chart repository
- helm uninstall (dev): uninstall the helm release; it will remove all of the resources associated with the last release of the chart.
- helm create (mychart): create a chart
- helm version: show both the client and server version
- helm init; // install on MacOs: brew install kubernetes-helm
- helm repo list
- helm repo search (mariadb)
- helm install stable/mariadb
- helm status // ???
- helm delete --deleted (--all)/ ???

For Kubernetes, it is equivalent to yum(Red Hat), apt(Debianã€Ubuntu), or homebrew(macOS, Linux).

A *release* represent an instance of a chart running in a Kubernetes cluster. Each release has its own unique name. 

[In Helm 3, Tiller will be removed](https://helm.sh/blog/helm-3-preview-pt2/), because the tiller inside a K8s cluster has too much poewr such as CREATE/UPDATE/DELETE and it causes some security issues. So, the Helm 3 client library will communicate directly with the Kubernetes API server not via Tiller.

 Without using Helm, you would typically need to maintain separate Kubernetes resource files (YAML manifests) for each environment, managing the differences in configurations, replica counts, secrets, and other environment-specific settings.

 Whereas with Helm, you can create a single chart for your application, which bundles all the necessary Kubernetes resources (deployments, services, ConfigMaps, secrets, etc.) into a unified package. Instead of modifying individual YAML files for each environment, you can just modify values.yaml file and deploy the application.

 Process:

- Creation of Helm Chart: includes templates for all required Kubernetes resources (deployments, services, ConfigMaps, secrets, etc.).
- Values.yaml: define the configurations for your application as per the environment
- Helm Renders Charts: During the deployment process, Helm renders the chart templates with the appropriate configuration for specific environments from values.yaml file and generate environment-specific Kubernetes manifest.
- Deploying to Kubernetes Cluster: Once Helm generates the environment-specific Kubernetes manifests they are deployed to the Kubernetes cluster.  According to the Helm request the Kubernetes APIs create or update the necessary resources based on configurations.

Advantage:

- Simplified Application Deployment: Helm packs all the necessary Kubernetes resources into a single package called a chart, making it easier to deploy and manage applications on Kubernetes.
- Consistent and Reliable Deployments: With the use of Helm charts, you can ensure consistent and reliable application deployment across environments (development, staging, production) by simply modifying the values.yaml file.
- Configuration Management: Helm separates the application configurations from deployment manifests, allowing you to manage configuration more effectively. You can easily update configurations by modifying the values.yaml file.
- Versioning and Rollbacks: Helm supports versioning of charts, allowing you to track changes and roll back to previous versions if needed, promoting better release management.

Challenges:

- Learning Curve: Helm CLI, being a command line utility, brings up a learning curve for users to understand and master ample commands. On the other hand, the templating structure itself is quite complex when it comes to creating a helm chart for complex microservices.
- Lack of Visibility and Monitoring: Helm CLI does not provide a built-in mechanism to view the health and status of applications deployed through its charts. Users need to rely on external monitoring tools or manually inspect the deployed resources to assess the state of their application

## Terraform

Terraform: Declarative Kubernetes Configurations

While Docker and Helm handle application packaging and Kubernetes deployments, managing the underlying infrastructure can be challenging. Terraform, an Infrastructure as Code (IaC) tool, bridges this gap by providing a declarative way to define, provision, and manage infrastructure resources.

With Terraform's Kubernetes provider, developers can create Kubernetes resources like namespaces, services, and secrets as code. This approach ensures that the infrastructure setup is repeatable, version-controlled, and easily shareable across teams. Additionally, by maintaining the state of the infrastructure, Terraform can automatically manage updates and ensure that the desired state matches the actual state.

Bonus - using the Helm Terraform provider, we can explicitly wire up dependencies between terraform resources, so that the output of a Terraform module can be used as an input to a Helm chart.

Terraform is a tool that allows us to safely and predictably manage infrastructure at scale using cloud-agnostic and infrastructure as code principles. It is a powerful tool developed by Hashicorp that enables infrastructure provisioning both on the cloud and on-premises. 

Terraform is written in a declarative configuration language, Hashicorp Configuration Language (HCL), and facilitates the automation of infrastructure management in any environment.

Modules provide excellent reusability and code-sharing opportunities to boost the collaboration and productivity of teams operating on the cloud. Providers are plugins that offer integration and interaction with different APIs and are one of the main ways to extend Terraformâ€™s functionality. 

Terraform keeps an internal state of the managed infrastructure, which represents resources, configuration, metadata, and their relationships. The state is actively maintained by Terraform and utilized to create plans, track changes, and enable modifications of infrastructure environments. The state should be stored remotely to allow teamwork and collaboration as a best practice.

The core Terraform workflow consists of three concrete stages. First, we generate the infrastructure as code configuration files representing our environmentâ€™s desired state. Next, we check the output of the generated plan based on our manifests. After carefully reviewing the changes, we apply the plan to provision infrastructure resources.

Terraform is an infrastructure provisioning tool that enables users to define and manage their infrastructure as code. It provides a declarative language to describe the desired state of infrastructure resources, such as virtual machines, Kubernetes nodes, storage, and networking components.

Terraform then automates the provisioning and management of these resources across multiple cloud platforms, making it highly versatile. By executing Terraform, the desired infrastructure is automatically provisioned, ensuring consistency and reproducibility.

## Terragrunt

Terragrunt: Orchestrating Terraform Configurations
As infrastructure grows in complexity, managing multiple Terraform configurations across different environments becomes challenging. Terragrunt, an open-source tool, acts as a thin wrapper around Terraform, enabling configuration reuse and better orchestration.

Terragrunt allows developers to follow the Don't Repeat Yourself (DRY) principle by sharing common Terraform configurations across different environments. With Terragrunt's ability to manage dependencies and inheritance, teams can maintain a clear and organized codebase while ensuring consistency and reducing errors.

## Multitenancy

Multitenancy (or multi-tenancy) refers to a single software installation that serves multiple tenants. A **tenant** is a user, application, or a group of users/applications that utilize the software to operate on their own data set. These tenants donâ€™t share data (unless explicitly instructed by the owner) and may not even be aware of one another.

A tenant can be as small as one independent user with a single login ID â€” think personal productivity software â€” or as large as an entire corporation with thousands of login IDs, each with its own privileges yet interrelated in multiple ways.

Without multitenancy, each tenant would need a dedicated software installation. This increases resource utilization and maintenance efforts, ultimately software costs.

Multitenant software provides each tenant a segregated environment (work data, settings, list of credentials, etc.), simultaneously serving multiple tenants. From a tenantâ€™s perspective, each has its dedicated software installation, although, in reality, they are all sharing one.

With multitenant software, tenants share the resources of one installation without affecting each other or only in predefined and controlled ways.

## YAML

Compared with XML and JSON, YAML is much simpler in presenting the same data.

Must have a space after colon ":" in Key-Value Pair. Sibling should have the same dents, but how many spaces do not matter.  

YAML in Kubernetes:

- always contains four keys and they are the root/top properties.
  - apiVersion: v1 of Kubernetes objects, like Pod
  - kind: type of object to create; e.g. Pod, Server, Deployment
  - metadata: data about the object; e.g. name, labels (dict); after labeling, it is easy to group these pods
  - spec: specification on the additional information for the object; depend on the object; a list of containers
- fe

can use some extension or plugin to hight the syntax error or validate the content 

---

æ‚¨æäº†ä¸€ä¸ªéå¸¸å¥½çš„è§‚ç‚¹ï¼ç¡®å®ï¼Œ**"Service"** è¿™ä¸ªè¯åœ¨ IT é¢†åŸŸå¤ªè¿‡é€šç”¨ï¼ˆä¾‹å¦‚ï¼ŒWeb Service, Windows Service, åç«¯æœåŠ¡ç­‰ï¼‰ï¼Œå¾ˆå®¹æ˜“è®©äººå¯¹ Kubernetes ä¸­çš„ `Service` äº§ç”Ÿå›°æƒ‘ã€‚

åœ¨ Kubernetes (K8s) ä¸­ï¼Œ`Service` çš„çœŸå®å«ä¹‰å’Œä½œç”¨ï¼Œä¸æˆ‘ä»¬æ—¥å¸¸è¯´çš„â€œæœåŠ¡â€æœ‰æ‰€ä¸åŒã€‚

---

## K8S Service çš„çœŸå®å«ä¹‰å’Œä½œç”¨

Kubernetes ä¸­çš„ **Service** æ˜¯ä¸€ç§ **æŠ½è±¡å±‚**ï¼Œå®ƒè§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ **Pod çš„ä¸ç¨³å®šæ€§å’Œå‘ç°æ€§**ã€‚

ç®€å•æ¥è¯´ï¼ŒK8s Service æœ‰ä¸¤ä¸ªä¸»è¦èŒè´£ï¼š

1. **è´Ÿè½½å‡è¡¡å™¨ (Load Balancer):** å°†å®¢æˆ·ç«¯çš„è¯·æ±‚åˆ†æ•£åˆ°å¤šä¸ªåç«¯çš„ Pod ä¸Šã€‚
2. **ç¨³å®šçš„ç½‘ç»œç«¯ç‚¹ (Stable Network Endpoint):** ä¸ºä¸€ç»„åŠ¨æ€å˜åŒ–çš„ Pod æä¾›ä¸€ä¸ªå›ºå®šçš„ IP åœ°å€å’Œ DNS åç§°ã€‚

### æ ¸å¿ƒç—›ç‚¹ä¸ Service çš„è§£å†³æ–¹æ¡ˆ

| ç—›ç‚¹ | K8s Service å¦‚ä½•è§£å†³ |
| :--- | :--- |
| **Pod æ˜¯ä¸´æ—¶çš„** | Pod éšæ—¶å¯èƒ½å› ä¸ºæ‰©å®¹ã€å‡çº§ã€é‡å¯ã€æ•…éšœç­‰åŸå› è¢«é”€æ¯å¹¶é‡å»ºï¼Œå…¶ IP åœ°å€ä¼šæ”¹å˜ã€‚ |
| **Service æä¾›äº†ç¨³å®šçš„ IP å’Œ DNS åç§°**ã€‚å‰ç«¯åº”ç”¨æˆ–å®¢æˆ·ç«¯ä¸éœ€è¦çŸ¥é“åç«¯ Pod çš„å…·ä½“ IPã€‚ |
| **Pod å‘ç°å›°éš¾** | å®¢æˆ·ç«¯éœ€è¦çŸ¥é“å“ªäº› Pod æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†è¯·æ±‚å‘é€åˆ°å¥åº·çš„ Pod ä¸Šã€‚ |
| **Service å……å½“â€œä»£ç†â€**ã€‚å®ƒä½¿ç”¨ **Selector (æ ‡ç­¾é€‰æ‹©å™¨)** è‡ªåŠ¨è·Ÿè¸ªå’Œç›‘æ§ä¸€ç»„ Podï¼Œå¹¶å°†è¯·æ±‚è·¯ç”±åˆ°è¿™äº› Podã€‚ |
| **è´Ÿè½½å‡è¡¡** | å®¢æˆ·ç«¯éœ€è¦å°†æµé‡å‡åŒ€åœ°åˆ†æ•£åˆ°æ‰€æœ‰å¥åº·çš„åç«¯ Pod ä¸Šã€‚ |
| **Service å†…ç½®è´Ÿè½½å‡è¡¡æœºåˆ¶**ï¼ˆé€šå¸¸æ˜¯ Round-Robin æˆ– Least Connectionsï¼‰ï¼Œç¡®ä¿æµé‡å‡åŒ€åˆ†é…ã€‚ |

### ä¸ºä»€ä¹ˆå«å®ƒ Serviceï¼ˆæœåŠ¡ï¼‰ï¼Ÿ

è™½ç„¶åå­—å¾ˆé€šç”¨ï¼Œä½† Kubernetes æ²¿ç”¨è¿™ä¸ªåç§°æ˜¯å› ä¸ºå®ƒç¡®å®å°†ä¸€ç»„ Pod æš´éœ²ä¸ºä¸€ä¸ªå¯ç”¨çš„â€œæœåŠ¡â€ï¼š

- **å¯¹å†…ï¼š** å®ƒä½¿é›†ç¾¤å†…çš„å…¶ä»– Pod å¯ä»¥é€šè¿‡ä¸€ä¸ªå›ºå®šçš„å†…éƒ¨ DNS åç§°è®¿é—®åç«¯åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œç›´æ¥è®¿é—® `my-database-service`ï¼‰ã€‚
- **å¯¹å¤–ï¼š** å®ƒå®šä¹‰äº†åº”ç”¨å¦‚ä½•ä»é›†ç¾¤å¤–éƒ¨ï¼ˆInternetï¼‰è¢«è®¿é—®ã€‚

æ‰€ä»¥ï¼Œä¸å…¶è¯´å®ƒæ˜¯ä¸€ä¸ªæœåŠ¡ï¼Œä¸å¦‚è¯´å®ƒæ˜¯ä¸€ä¸ª **â€œæœåŠ¡æŠ½è±¡/å‘ç°å±‚â€** æˆ– **â€œç½‘ç»œä»£ç†â€**ã€‚

---

### K8s Service çš„ç±»å‹

Kubernetes Service æœ‰å‡ ç§ä¸åŒçš„ç±»å‹ï¼Œå†³å®šäº†å®ƒå¦‚ä½•å°†åº”ç”¨æš´éœ²ç»™ç½‘ç»œï¼š

| Service ç±»å‹ | ä½œç”¨ | æš´éœ²èŒƒå›´ |
| :--- | :--- | :--- |
| **ClusterIP (é»˜è®¤)** | ä¸º Service æä¾›ä¸€ä¸ªé›†ç¾¤å†…éƒ¨çš„ IP åœ°å€ï¼Œ**åªèƒ½**åœ¨é›†ç¾¤å†…éƒ¨è®¿é—®ã€‚ | ä»…é™é›†ç¾¤å†…éƒ¨ |
| **NodePort** | åœ¨é›†ç¾¤çš„æ¯ä¸ªèŠ‚ç‚¹ (Node) ä¸Šæ‰“å¼€ä¸€ä¸ªé™æ€ç«¯å£ï¼Œå¤–éƒ¨æµé‡å¯ä»¥é€šè¿‡ `<NodeIP>:<NodePort>` è®¿é—®ã€‚ | æš´éœ²ç»™å¤–éƒ¨ |
| **LoadBalancer** | ä»…åœ¨äº‘ç¯å¢ƒä¸­å¯ç”¨ã€‚å®ƒä¼šè‡ªåŠ¨åœ¨äº‘å¹³å° (å¦‚ AWS, GCP, Azure) ä¸Šé…ç½®ä¸€ä¸ªå¤–éƒ¨çš„äº‘è´Ÿè½½å‡è¡¡å™¨ï¼Œå¹¶å°†æµé‡å¯¼å…¥é›†ç¾¤ã€‚ | æš´éœ²ç»™å¤–éƒ¨ (æ¨è) |
| **ExternalName** | å°† Service æ˜ å°„åˆ°ä¸€ä¸ªå¤–éƒ¨çš„ DNS åç§°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªé›†ç¾¤å†…éƒ¨ IPã€‚ | å¤–éƒ¨ DNS |

æ€»è€Œè¨€ä¹‹ï¼Œæ‚¨å¯¹ `Service` åå­—çš„å›°æƒ‘æ˜¯åˆç†çš„ï¼Œä½†è¯·è®°ä½å®ƒåœ¨ K8s ä¸­çš„ç‰¹å®šå«ä¹‰ï¼š**ä¸€ä¸ªä¸ºä¸€ç»„åç«¯ Pod æä¾›ç¨³å®šç½‘ç»œè®¿é—®å’Œè´Ÿè½½å‡è¡¡çš„æŠ½è±¡å±‚ã€‚**

è¿™æ˜¯ä¸€ä¸ªç†è§£ Kubernetes æ ¸å¿ƒæœºåˆ¶çš„å…³é”®é—®é¢˜ã€‚**Label (æ ‡ç­¾)** å’Œ **Selector (é€‰æ‹©å™¨)** æ˜¯ Kubernetes ä¸­ç”¨äºç»„ç»‡ã€ç®¡ç†å’Œé€‰æ‹©èµ„æºçš„åŸºæœ¬æœºåˆ¶ã€‚å®ƒä»¬å…±åŒåä½œï¼Œå®ç°äº† Kubernetes å¼ºå¤§çš„è‡ªåŠ¨åŒ–å’Œè§£è€¦èƒ½åŠ›ã€‚

---

### 1. Labelï¼ˆæ ‡ç­¾ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ

**Label** æ˜¯é™„åŠ åˆ° Kubernetes å¯¹è±¡ï¼ˆå¦‚ Podã€Nodeã€Serviceã€Deployment ç­‰ï¼‰ä¸Šçš„ **é”®å€¼å¯¹ (Key-Value pairs)**ã€‚

#### ä½œç”¨ï¼šç»„ç»‡å’Œè¯†åˆ«

- **è¯†åˆ«å±æ€§ï¼š** Label ç”¨äºä»¥å¯¹ç”¨æˆ·æœ‰æ„ä¹‰çš„æ–¹å¼è¯†åˆ«å¯¹è±¡ã€‚
- **è§£è€¦ï¼š** å®ƒä»¬ä¸ç›´æ¥è€¦åˆåˆ°æ ¸å¿ƒç³»ç»Ÿï¼Œè€Œæ˜¯ç”¨äºç”¨æˆ·è‡ªå®šä¹‰çš„ç»„ç»‡ç»“æ„ã€‚
- **ç¤ºä¾‹ï¼š**
  - **åº”ç”¨ç±»å‹ï¼š** `app: frontend` æˆ– `app: database`
  - **ç¯å¢ƒï¼š** `env: production` æˆ– `env: development`
  - **ç‰ˆæœ¬ï¼š** `version: v1.2.3`

**ç‰¹ç‚¹ï¼š**

- ä¸€ä¸ªå¯¹è±¡å¯ä»¥æœ‰å¤šä¸ª Labelã€‚
- Label å¯ä»¥åœ¨å¯¹è±¡åˆ›å»ºåéšæ—¶æ·»åŠ æˆ–ä¿®æ”¹ã€‚

---

### 2. Selectorï¼ˆé€‰æ‹©å™¨ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ

**Selector** æ˜¯ä¸€ç§è¡¨è¾¾å¼ï¼Œç”¨äº **åŒ¹é…** å…·æœ‰ç‰¹å®š Label çš„ä¸€ç»„å¯¹è±¡ã€‚å®ƒæ˜¯ Kubernetes è‡ªåŠ¨åŒ–å’Œæ§åˆ¶å¾ªç¯å·¥ä½œçš„æ ¸å¿ƒæœºåˆ¶ã€‚

#### ä½œç”¨ï¼šå®šä½å’Œç®¡ç†

Selector çš„ä½œç”¨æ˜¯å‘Šè¯‰æ§åˆ¶å™¨ï¼ˆControllerï¼‰æˆ– Service åº”è¯¥ç®¡ç†æˆ–è·¯ç”±åˆ°å“ªäº›å¯¹è±¡ã€‚

#### å¸¸è§çš„ Selector ä½¿ç”¨åœºæ™¯

1. **ReplicaSet/Deployment Selector:** ç”¨äºå‘Šè¯‰æ§åˆ¶å™¨åº”è¯¥ç®¡ç†å“ªäº› Podï¼ˆç¡®ä¿å®ƒä»¬æ•°é‡æ­£ç¡®ï¼‰ã€‚
2. **Service Selector:** ç”¨äºå‘Šè¯‰ Service åº”è¯¥å°†å®¢æˆ·ç«¯è¯·æ±‚è·¯ç”±åˆ°å“ªäº›åç«¯çš„ Podã€‚
3. **Node Selector:** ç”¨äºé™åˆ¶ Pod åªèƒ½åœ¨å…·æœ‰ç‰¹å®š Label çš„ Node ä¸Šè°ƒåº¦ã€‚

**ç¤ºä¾‹ï¼š**

ä¸€ä¸ª Service çš„ Selector å¯èƒ½æ˜¯ `app: frontend`ï¼Œè¿™æ„å‘³ç€å®ƒåªä¼šå°†æµé‡è·¯ç”±åˆ°æ‰€æœ‰å¸¦æœ‰ `app: frontend` æ ‡ç­¾çš„ Podã€‚

---

### 3. Label å’Œ Selector çš„å…³ç³»ï¼ˆæ ¸å¿ƒï¼‰

**Label å’Œ Selector ä¹‹é—´çš„å…³ç³»æ˜¯â€œæ ‡è¯†ä¸é€‰æ‹©â€çš„å…³ç³»ã€‚**

* **Label è´Ÿè´£æ ‡è¯† (Identity):** å®ƒä»¬æ˜¯å¯¹è±¡èº«ä¸Šçš„â€œèº«ä»½æ ‡ç­¾â€ã€‚
* **Selector è´Ÿè´£é€‰æ‹© (Selection):** å®ƒä»¬æ˜¯â€œæŸ¥æ‰¾æ¡ä»¶â€ï¼Œæ ¹æ® Label å»æ‰¾å¯¹åº”çš„å¯¹è±¡ã€‚

#### å…³ç³»å›¾ç¤º

| èµ„æº | å®šä¹‰ Selector | å®šä¹‰ Label | æ ¸å¿ƒåŠŸèƒ½ |
| :--- | :--- | :--- | :--- |
| **Deployment / ReplicaSet** | `spec.selector.matchLabels` | `spec.template.metadata.labels` | **ç®¡ç† Podï¼š** ReplicaSet ä½¿ç”¨ Selector æ‰¾åˆ°ç”±å®ƒç®¡ç†çš„ Podï¼Œå¹¶ç¡®ä¿æ•°é‡ç­‰äº `replicas`ã€‚ |
| **Service** | `spec.selector` | ä½œç”¨äº Pod | **æµé‡è·¯ç”±ï¼š** Service ä½¿ç”¨ Selector æ‰¾åˆ°åç«¯å¥åº·çš„ Pod åˆ—è¡¨ï¼Œå¹¶å°†æµé‡è´Ÿè½½å‡è¡¡åˆ°è¿™äº› Podã€‚ |

**å…³é”®çš„åŒ¹é…åŸåˆ™ï¼š**

1. **æ§åˆ¶å™¨å’Œ Pod æ¨¡æ¿ï¼š** åœ¨å®šä¹‰åƒ `Deployment` è¿™æ ·çš„æ§åˆ¶å™¨æ—¶ï¼Œæ‚¨å¿…é¡»ç¡®ä¿ `spec.selector.matchLabels` ä¸­å®šä¹‰çš„æ ‡ç­¾é€‰æ‹©å™¨ï¼Œä¸ `spec.template.metadata.labels` ä¸­å®šä¹‰çš„ Pod æ ‡ç­¾**å®Œå…¨ä¸€è‡´**ã€‚å¦‚æœå®ƒä»¬ä¸åŒ¹é…ï¼Œæ§åˆ¶å™¨å°†æ— æ³•æ‰¾åˆ°æˆ–åˆ›å»ºè‡ªå·±çš„ Podã€‚
2. **Pod å’Œ Serviceï¼š** Service ä½¿ç”¨å…¶ Selector æ¥æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ Podï¼Œè¿™äº› Pod æˆä¸º Service çš„**ç«¯ç‚¹ (Endpoints)**ã€‚

---

## äº‘åŸç”Ÿ (Cloud Native) 

è¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¹¿æ³›ä¸”éå¸¸é‡è¦çš„æ¦‚å¿µã€‚**äº‘åŸç”Ÿ (Cloud Native)** æ˜¯ä¸€å¥—è®¾è®¡ã€æ„å»ºå’Œè¿è¡Œåº”ç”¨ç¨‹åºçš„æ–¹æ³•è®ºï¼Œå®ƒå……åˆ†åˆ©ç”¨äº†äº‘è®¡ç®—å¹³å°çš„ä¼˜åŠ¿ï¼ˆä¾‹å¦‚å¼¹æ€§ã€åˆ†å¸ƒå¼ã€å¯æ‰©å±•æ€§ï¼‰ã€‚

å®ƒä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„æŠ€æœ¯ï¼Œè€Œæ˜¯ä¸€ç³»åˆ—æ–‡åŒ–ã€æ¶æ„å’ŒæŠ€æœ¯å®è·µçš„é›†åˆã€‚

---

### äº‘åŸç”Ÿçš„æ ¸å¿ƒä¸‰è¦ç´ 

äº‘åŸç”Ÿç”Ÿæ€ç³»ç»Ÿç”±ä»¥ä¸‹ä¸‰ä¸ªç´§å¯†ç›¸è¿çš„æ”¯æŸ±æ”¯æ’‘ï¼š

#### 1. æŠ€æœ¯æ¶æ„ (Architecture)

è¿™æ˜¯äº‘åŸç”Ÿçš„åŸºç¡€ï¼Œè¦æ±‚åº”ç”¨ç¨‹åºåœ¨è®¾è®¡ä¹‹åˆå°±è€ƒè™‘åˆ°äº‘è®¡ç®—ç¯å¢ƒçš„ç‰¹ç‚¹ã€‚

| å®è·µåç§° | æ ¸å¿ƒæ¦‚å¿µ | ç›®çš„å’Œä¼˜åŠ¿ |
| :--- | :--- | :--- |
| **å¾®æœåŠ¡ (Microservices)** | å°†ä¸€ä¸ªåºå¤§çš„å•ä½“åº”ç”¨æ‹†åˆ†æˆä¸€ç»„å°å‹çš„ã€ç‹¬ç«‹éƒ¨ç½²å’Œç‹¬ç«‹è¿è¡Œçš„æœåŠ¡ï¼Œæ¯ä¸ªæœåŠ¡åªå…³æ³¨ä¸€ä¸ªä¸šåŠ¡åŠŸèƒ½ã€‚ | æé«˜å¼€å‘é€Ÿåº¦ï¼Œå…è®¸ä½¿ç”¨ä¸åŒçš„æŠ€æœ¯æ ˆï¼Œæ•…éšœéš”ç¦»ã€‚ |
| **å®¹å™¨åŒ– (Containerization)** | ä½¿ç”¨ Docker ç­‰æŠ€æœ¯å°†åº”ç”¨åŠå…¶æ‰€æœ‰ä¾èµ–é¡¹ï¼ˆä»£ç ã€è¿è¡Œæ—¶ã€ç³»ç»Ÿå·¥å…·ï¼‰æ‰“åŒ…æˆä¸€ä¸ªç‹¬ç«‹çš„ã€å¯ç§»æ¤çš„å•å…ƒã€‚ | ç¡®ä¿åº”ç”¨åœ¨ä»»ä½•ç¯å¢ƒï¼ˆå¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ï¼‰ä¸­çš„ä¸€è‡´æ€§è¿è¡Œã€‚ |
| **ä¸å¯å˜åŸºç¡€è®¾æ–½ (Immutable Infrastructure)** | åŸºç¡€è®¾æ–½ä¸€æ—¦éƒ¨ç½²å°±ä¸èƒ½ä¿®æ”¹ã€‚å¦‚æœéœ€è¦æ›´æ–°ï¼Œåˆ™é”€æ¯æ—§å®ä¾‹ï¼Œéƒ¨ç½²æ–°å®ä¾‹ã€‚ | æé«˜å¯é æ€§ã€å¯é¢„æµ‹æ€§å’Œå®‰å…¨æ€§ã€‚ |

#### 2. è‡ªåŠ¨åŒ–å’Œç®¡ç† (Automation & Management)

è¿™æ˜¯å®ç°äº‘åŸç”Ÿæ•ˆç‡çš„å…³é”®ï¼Œéœ€è¦å·¥å…·æ¥ç®¡ç†å’Œç¼–æ’å®¹å™¨ã€è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œèµ„æºè°ƒåº¦ã€‚

| å®è·µåç§° | æ ¸å¿ƒæ¦‚å¿µ | ç›®çš„å’Œä¼˜åŠ¿ |
| :--- | :--- | :--- |
| **Kubernetes (K8s)** | å®¹å™¨ç¼–æ’ç³»ç»Ÿï¼Œè´Ÿè´£è‡ªåŠ¨åŒ–éƒ¨ç½²ã€æ‰©å±•å’Œç®¡ç†å®¹å™¨åŒ–åº”ç”¨ç¨‹åºã€‚ | è‡ªåŠ¨è°ƒåº¦ã€è‡ªæ„ˆèƒ½åŠ›ã€é«˜æ•ˆåˆ©ç”¨èµ„æºï¼Œæ˜¯äº‘åŸç”Ÿçš„**åŸºç¡€è®¾æ–½æ ¸å¿ƒ**ã€‚ |
| **æœåŠ¡ç½‘æ ¼ (Service Mesh)** | æä¾›ç½‘ç»œæ§åˆ¶å±‚ï¼ˆå¦‚ Istioï¼‰ï¼Œå¤„ç†æœåŠ¡ä¹‹é—´çš„é€šä¿¡ã€å®‰å…¨ã€ç­–ç•¥å’Œå¯è§‚æµ‹æ€§ï¼Œå°†è¿™äº›é€»è¾‘ä»åº”ç”¨ä»£ç ä¸­å‰¥ç¦»ã€‚ | ç®€åŒ–å¾®æœåŠ¡æ²»ç†ï¼Œæé«˜é€šä¿¡çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚ |

#### 3. æ–‡åŒ–å’Œæµç¨‹ (Culture & Process)

æŠ€æœ¯ä¸Šçš„æ”¹å˜å¿…é¡»ä¼´éšç€æ–‡åŒ–å’Œæµç¨‹çš„é©æ–°ï¼Œæ‰èƒ½çœŸæ­£å‘æŒ¥äº‘åŸç”Ÿçš„æ½œåŠ›ã€‚

| å®è·µåç§° | æ ¸å¿ƒæ¦‚å¿µ | ç›®çš„å’Œä¼˜åŠ¿ |
| :--- | :--- | :--- |
| **DevOps** | å¼ºè°ƒå¼€å‘ (Dev) å’Œè¿ç»´ (Ops) å›¢é˜Ÿä¹‹é—´çš„åä½œã€æ²Ÿé€šå’Œé›†æˆã€‚ | åŠ å¿«è½¯ä»¶äº¤ä»˜é€Ÿåº¦å’Œå¯é æ€§ã€‚ |
| **æŒç»­é›†æˆ/æŒç»­äº¤ä»˜ (CI/CD)** | è‡ªåŠ¨åŒ–æ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²æµç¨‹ï¼Œä½¿ä»£ç å˜æ›´èƒ½å¤Ÿå¿«é€Ÿã€å¯é åœ°å‘å¸ƒåˆ°ç”Ÿäº§ç¯å¢ƒã€‚ | å®ç°å¿«é€Ÿè¿­ä»£å’ŒæŒç»­æ”¹è¿›ã€‚ |
| **å¯è§‚æµ‹æ€§ (Observability)** | ä¸ä»…ä»…æ˜¯ç›‘æ§ï¼Œè€Œæ˜¯é€šè¿‡æ”¶é›†å’Œåˆ†ææ—¥å¿— (Logs)ã€æŒ‡æ ‡ (Metrics) å’Œé“¾è·¯è¿½è¸ª (Tracing)ï¼Œå®æ—¶äº†è§£ç³»ç»Ÿçš„å†…éƒ¨çŠ¶æ€ã€‚ | å¿«é€Ÿå‘ç°ã€è¯Šæ–­å’Œè§£å†³å¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿé—®é¢˜ã€‚ |

---

### äº‘åŸç”Ÿçš„æœ€ç»ˆç›®æ ‡

äº‘åŸç”Ÿçš„æœ€ç»ˆç›®æ ‡æ˜¯ï¼š

**æœ€å¤§åŒ–é€Ÿåº¦ã€æ•æ·æ€§å’Œå¯é æ€§ï¼Œå®ç°å¿«é€Ÿã€æŒç»­ã€å¤§è§„æ¨¡åœ°äº¤ä»˜é«˜è´¨é‡çš„åº”ç”¨ç¨‹åºå’ŒæœåŠ¡ã€‚**

å®ƒå°†äº‘è®¡ç®—çš„èƒ½åŠ›ï¼ˆå¼¹æ€§ã€æŒ‰éœ€ä»˜è´¹ã€å…¨çƒåˆ†å¸ƒï¼‰ä¸ç°ä»£è½¯ä»¶å¼€å‘å®è·µï¼ˆå¾®æœåŠ¡ã€è‡ªåŠ¨åŒ–ï¼‰ç›¸ç»“åˆï¼Œè®©ä¼ä¸šèƒ½å¤Ÿåƒ Googleã€Netflix é‚£æ ·è¿è¥å…¶è½¯ä»¶ã€‚

---

## Service Meshï¼ˆæœåŠ¡ç½‘æ ¼ï¼‰ä»‹ç»

**Service Mesh**ï¼ˆæœåŠ¡ç½‘æ ¼ï¼‰æ˜¯ä¸€ç§ä¸“ç”¨äºå¤„ç†**æœåŠ¡é—´é€šä¿¡**çš„åŸºç¡€è®¾æ–½å±‚ã€‚å®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯å°†å¾®æœåŠ¡æ¶æ„ä¸­å¤æ‚çš„ç½‘ç»œå’ŒæœåŠ¡æ²»ç†åŠŸèƒ½ï¼ˆå¦‚æµé‡æ§åˆ¶ã€å®‰å…¨ã€å¯è§‚æµ‹æ€§ï¼‰ä»**åº”ç”¨ç¨‹åºä»£ç ä¸­å‰¥ç¦»å‡ºæ¥**ï¼Œä¸‹æ²‰åˆ°åŸºç¡€è®¾æ–½å±‚ç»Ÿä¸€ç®¡ç†ã€‚

ç®€å•æ¥è¯´ï¼ŒService Mesh å°±åƒæ˜¯ä¸ºå¾®æœåŠ¡åº”ç”¨ä¸­çš„æ¯ä¸€ä¸ªæœåŠ¡å®ä¾‹éƒ¨ç½²äº†ä¸€ä¸ª**æ™ºèƒ½ä»£ç†ç½‘ç»œ**ã€‚

### æ ¸å¿ƒç—›ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

åœ¨ä¼ ç»Ÿçš„å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œæ¯ä¸€ä¸ªæœåŠ¡éƒ½éœ€è¦è‡ªå·±å¤„ç†ä»¥ä¸‹å¤æ‚çš„é€»è¾‘ï¼š

| ä¼ ç»Ÿç—›ç‚¹ | æœåŠ¡ç½‘æ ¼è§£å†³æ–¹æ¡ˆ |
| :--- | :--- |
| **ç½‘ç»œå¤æ‚æ€§** | æœåŠ¡éœ€è¦å¤„ç†é‡è¯•ã€è¶…æ—¶ã€æ–­è·¯å™¨ã€è´Ÿè½½å‡è¡¡ç­‰é€»è¾‘ã€‚ | **ä»£ç†æ¥ç®¡ï¼š** è¿™äº›é€»è¾‘è¢«è½¬ç§»åˆ°ä»£ç†ä¸­ï¼Œåº”ç”¨ç¨‹åºä»£ç ä¿æŒâ€œå¹²å‡€â€ã€‚ |
| **å®‰å…¨æ€§** | æœåŠ¡é—´éœ€è¦æ‰‹åŠ¨é…ç½® mTLSï¼ˆåŒå‘ TLSï¼‰åŠ å¯†é€šä¿¡ã€‚ | **è‡ªåŠ¨åŒ– mTLSï¼š** ä»£ç†è‡ªåŠ¨å¤„ç†è¯ä¹¦å’ŒåŠ å¯†ï¼Œç¡®ä¿æ‰€æœ‰æµé‡é»˜è®¤åŠ å¯†ã€‚ |
| **å¯è§‚æµ‹æ€§** | éš¾ä»¥è¿½è¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„è¯·æ±‚é“¾è·¯å’Œæ€§èƒ½æŒ‡æ ‡ã€‚ | **ç»Ÿä¸€é¥æµ‹ï¼š** ä»£ç†è‡ªåŠ¨æ”¶é›†æ‰€æœ‰æµé‡çš„æŒ‡æ ‡ï¼ˆMetricsï¼‰ã€æ—¥å¿—ï¼ˆLogsï¼‰å’Œé“¾è·¯è¿½è¸ªï¼ˆTracingï¼‰æ•°æ®ã€‚ |

---

### Service Mesh çš„æ¶æ„

Service Mesh é€šå¸¸ç”±ä¸¤å¤§éƒ¨åˆ†ç»„æˆï¼š

#### 1. æ•°æ®å¹³é¢ (Data Plane)

- **ç»„æˆï¼š** ç”±ä¸€ç»„é«˜æ€§èƒ½ã€è½»é‡çº§çš„ **ç½‘ç»œä»£ç† (Proxy)** ç»„æˆï¼Œè¿™äº›ä»£ç†é€šå¸¸è¢«ç§°ä¸º **Sidecar ä»£ç†**ã€‚
- **å·¥ä½œæ–¹å¼ï¼š** æ¯ä¸ªå¾®æœåŠ¡å®ä¾‹ï¼ˆåœ¨ Kubernetes ä¸­å°±æ˜¯ä¸€ä¸ª Podï¼‰éƒ½ä¼šä¼´éšéƒ¨ç½²ä¸€ä¸ª Sidecar ä»£ç†ã€‚æ‰€æœ‰è¿›å‡ºè¯¥æœåŠ¡ Pod çš„ç½‘ç»œæµé‡éƒ½å¿…é¡»ç»è¿‡è¿™ä¸ª Sidecar ä»£ç†ã€‚
- **åŠŸèƒ½ï¼š** è´Ÿè½½å‡è¡¡ã€è¯·æ±‚è·¯ç”±ã€æµé‡æ§åˆ¶ã€åŠ å¯†è§£å¯†ã€æŒ‡æ ‡æ”¶é›†ã€‚
- **ä¸»æµæŠ€æœ¯ï¼š** **Envoy** æ˜¯ç›®å‰æœ€æµè¡Œçš„ Sidecar ä»£ç†æŠ€æœ¯ã€‚

#### 2. æ§åˆ¶å¹³é¢ (Control Plane)

- **ç»„æˆï¼š** ä¸€ç»„ç®¡ç†å’Œé…ç½®æ‰€æœ‰ Sidecar ä»£ç†çš„ç»„ä»¶ã€‚
- **å·¥ä½œæ–¹å¼ï¼š** æ¥æ”¶æ¥è‡ªæ“ä½œå‘˜çš„é…ç½®ï¼ˆä¾‹å¦‚ï¼šâ€œå°† 10% çš„æµé‡è·¯ç”±åˆ°æ–°ç‰ˆæœ¬â€ï¼‰ï¼Œå¹¶å°†è¿™äº›é…ç½®å®æ—¶åˆ†å‘ç»™æ•°æ®å¹³é¢ä¸­çš„æ‰€æœ‰ Sidecar ä»£ç†ã€‚
- **åŠŸèƒ½ï¼š** é…ç½®ç®¡ç†ã€æœåŠ¡å‘ç°ã€è¯ä¹¦ç®¡ç†ã€ç­–ç•¥æ‰§è¡Œã€‚
- **ä¸»æµæŠ€æœ¯ï¼š** **Istio**ã€**Linkerd**ã€‚

---

### Service Mesh çš„ä¸»è¦ä¼˜åŠ¿

| åŠŸèƒ½ç±»åˆ« | æè¿° | ç¤ºä¾‹åº”ç”¨åœºæ™¯ |
| :--- | :--- | :--- |
| **æµé‡ç®¡ç†** | å®ç°é«˜çº§è·¯ç”±å’Œæµé‡è½¬ç§»ã€‚ | **é‡‘ä¸é›€å‘å¸ƒ (Canary Release)**ï¼šå°† 5% çš„ç”¨æˆ·æµé‡å¯¼å‘æ–°ç‰ˆæœ¬ï¼Œæ— é£é™©éªŒè¯ã€‚ |
| **å®‰å…¨æ€§** | å¼ºåˆ¶æ‰§è¡ŒæœåŠ¡é—´é€šä¿¡çš„èº«ä»½éªŒè¯å’Œæˆæƒã€‚ | è‡ªåŠ¨ä¸ºæ‰€æœ‰æœåŠ¡å¯ç”¨ **åŒå‘ TLS (mTLS)**ï¼Œç¡®ä¿åªæœ‰ä¿¡ä»»çš„æœåŠ¡å¯ä»¥é€šä¿¡ã€‚ |
| **å¯è§‚æµ‹æ€§** | è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„é¥æµ‹æ•°æ®ã€‚ | è‡ªåŠ¨ç”Ÿæˆè¯·æ±‚çš„ **ç«¯åˆ°ç«¯é“¾è·¯è¿½è¸ª**ï¼Œå¿«é€Ÿå®šä½åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚ |
| **å¼¹æ€§** | æä¾›å†…ç½®çš„æ–­è·¯å™¨å’Œé‡è¯•æœºåˆ¶ã€‚ | å¦‚æœåç«¯æœåŠ¡å“åº”é”™è¯¯ï¼Œä»£ç†è‡ªåŠ¨è§¦å‘ **æ–­è·¯å™¨**ï¼Œé¿å…é›ªå´©æ•ˆåº”ã€‚ |

Service Mesh çš„å‡ºç°æ ‡å¿—ç€å¾®æœåŠ¡æ²»ç†è¿›å…¥äº†ä¸€ä¸ªæ–°çš„é˜¶æ®µï¼Œå®ƒè®©åº”ç”¨å›¢é˜Ÿèƒ½å¤Ÿæ›´ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘çš„å®ç°

---

## Istio

å¥½çš„ï¼ŒIstio æ˜¯ç›®å‰åœ¨äº‘åŸç”Ÿé¢†åŸŸåº”ç”¨æœ€å¹¿æ³›ã€åŠŸèƒ½æœ€å¼ºå¤§çš„ **Service Meshï¼ˆæœåŠ¡ç½‘æ ¼ï¼‰** å¹³å°ã€‚å®ƒè¢«è®¾è®¡ç”¨æ¥è§£å†³å¤§è§„æ¨¡å¾®æœåŠ¡æ¶æ„ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ Kubernetes ç¯å¢ƒä¸­ã€‚

### Istio çš„æ ¸å¿ƒå®šä½

Istio çš„æ ¸å¿ƒä»·å€¼åœ¨äºï¼Œå®ƒæä¾›äº†ä¸€ä¸ª**é€æ˜çš„ã€è¯­è¨€æ— å…³çš„**åŸºç¡€è®¾æ–½å±‚ï¼Œç”¨äºç®¡ç†æœåŠ¡é—´é€šä¿¡ã€‚è¿™ä½¿å¾—å¼€å‘è€…å¯ä»¥å°†ç²¾åŠ›é›†ä¸­åœ¨ä¸šåŠ¡é€»è¾‘ä¸Šï¼Œè€Œå°†æœåŠ¡æ²»ç†ã€å®‰å…¨å’Œå¯è§‚æµ‹æ€§ç­‰å¤æ‚é—®é¢˜äº¤ç»™ Istio å¹³å°å¤„ç†ã€‚

#### Istio çš„ä¸‰å¤§ç›®æ ‡

1. **è¿æ¥ (Connect):** å®ç°æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡ã€‚
2. **å®‰å…¨ (Secure):** é»˜è®¤å¯ç”¨æœåŠ¡é—´é€šä¿¡çš„èº«ä»½éªŒè¯å’Œæˆæƒã€‚
3. **æ§åˆ¶/å¯è§‚æµ‹ (Control & Observe):** å®æ–½ç­–ç•¥ï¼Œå¹¶ä»ç½‘ç»œæµé‡ä¸­æå–è¯¦ç»†çš„é¥æµ‹æ•°æ®ã€‚

---

### Istio çš„æ¶æ„ç»„æˆ

Istio çš„æ¶æ„éµå¾ªå…¸å‹çš„ Service Mesh æ¨¡å¼ï¼Œç”±ä¸¤ä¸ªä¸»è¦çš„å¹³é¢ç»„æˆï¼š**æ•°æ®å¹³é¢** å’Œ **æ§åˆ¶å¹³é¢**ã€‚

#### 1. æ•°æ®å¹³é¢ (Data Plane)

- **ç»„ä»¶ï¼š** ä¸»è¦æ˜¯ **Envoy ä»£ç†**ã€‚
- **å·¥ä½œæ–¹å¼ï¼š** åœ¨ Kubernetes ä¸­ï¼ŒIstio é€šè¿‡ **Sidecar æ¨¡å¼**ï¼Œå°†ä¸€ä¸ª Envoy ä»£ç†æ³¨å…¥åˆ°æ¯ä¸ªåº”ç”¨ Pod ä¸­ã€‚æ‰€æœ‰è¿›å‡ºè¯¥ Pod çš„ç½‘ç»œæµé‡éƒ½ä¼šè¢« Envoy ä»£ç†æ‹¦æˆªå’Œå¤„ç†ã€‚
- **åŠŸèƒ½ï¼š** è´Ÿè½½å‡è¡¡ã€å¥åº·æ£€æŸ¥ã€é‡è¯•/è¶…æ—¶ã€ç†”æ–­ï¼ˆæ–­è·¯å™¨ï¼‰ã€æµé‡è·¯ç”±ã€æŒ‡æ ‡æ”¶é›†ç­‰ã€‚

###$ 2. æ§åˆ¶å¹³é¢ (Control Plane)

- **ç»„ä»¶ï¼š** **Istiod**ï¼ˆè¿™æ˜¯ Istio 1.5 ç‰ˆæœ¬åæ•´åˆçš„æ ¸å¿ƒå•ä½“ç»„ä»¶ï¼‰ã€‚
- **å·¥ä½œæ–¹å¼ï¼š** Istiod è´Ÿè´£å°†ç”¨æˆ·å®šä¹‰çš„é…ç½®è§„åˆ™ï¼ˆé€šè¿‡ Istio çš„è‡ªå®šä¹‰èµ„æºå¯¹è±¡å¦‚ `VirtualService`ï¼‰è½¬åŒ–ä¸º Envoy ä»£ç†èƒ½å¤Ÿç†è§£çš„é…ç½®ï¼Œå¹¶å®æ—¶ä¸‹å‘åˆ°æ•°æ®å¹³é¢ä¸­çš„æ‰€æœ‰ Sidecar ä»£ç†ã€‚
- **åŠŸèƒ½ï¼š**
  - **é…ç½®ç®¡ç†ï¼š** ç®¡ç†æµé‡è§„åˆ™ã€å®‰å…¨ç­–ç•¥å’Œ Mixerï¼ˆæ—§ç‰ˆï¼‰ç­–ç•¥ã€‚
  - **è¯ä¹¦ç®¡ç†ï¼š** ä¸ºæœåŠ¡è‡ªåŠ¨é¢å‘å’Œè½®æ¢è¯ä¹¦ï¼Œå®ç° mTLSã€‚
  - **æœåŠ¡å‘ç°ï¼š** çŸ¥é“é›†ç¾¤ä¸­æ‰€æœ‰æœåŠ¡çš„ç½‘ç»œä½ç½®ã€‚

---

### Istio çš„å…³é”®åŠŸèƒ½è¯¦è§£

Istio æä¾›çš„åŠŸèƒ½éå¸¸ä¸°å¯Œï¼Œå…¶ä¸­æœ€å¸¸ç”¨çš„åŒ…æ‹¬ï¼š

#### 1. æµé‡ç®¡ç† (Traffic Management)

è¿™æ˜¯ Istio æœ€æ ¸å¿ƒçš„åŠŸèƒ½ä¹‹ä¸€ï¼Œå®ƒä½¿ç”¨ **è‡ªå®šä¹‰èµ„æº (CRD)** æ¥æ§åˆ¶æµé‡è¡Œä¸ºï¼š

- **VirtualService (è™šæ‹ŸæœåŠ¡):** å®šä¹‰å¦‚ä½•å°†è¯·æ±‚è·¯ç”±åˆ°ç›®æ ‡æœåŠ¡ï¼Œæ”¯æŒåŸºäº Headerã€æƒé‡ã€æ¥æºç­‰å¤æ‚è§„åˆ™ã€‚
  - **åº”ç”¨åœºæ™¯ï¼š** **é‡‘ä¸é›€å‘å¸ƒ (Canary Release)**ã€A/B æµ‹è¯•ã€‚
- **DestinationRule (ç›®æ ‡è§„åˆ™):** å®šä¹‰è·¯ç”±åçš„ç›®æ ‡æœåŠ¡çš„å­é›†å’Œè´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚
- **Gateway (ç½‘å…³):** ç®¡ç†è¿›å‡ºé›†ç¾¤çš„å¤–éƒ¨æµé‡ï¼ˆå—åŒ—å‘æµé‡ï¼‰ã€‚

#### 2. å®‰å…¨ (Security)

Istio é»˜è®¤æä¾›å¼ºå¤§çš„å®‰å…¨åŠŸèƒ½ï¼Œæ— éœ€ä¿®æ”¹åº”ç”¨ä»£ç ï¼š

- **èº«ä»½è®¤è¯ (Authentication):**
  - **æœåŠ¡åˆ°æœåŠ¡ï¼š** è‡ªåŠ¨å¯ç”¨ **åŒå‘ TLS (mTLS)**ï¼ŒåŠ å¯†æœåŠ¡é—´é€šä¿¡ã€‚
  - **ç»ˆç«¯ç”¨æˆ·ï¼š** æ”¯æŒ JWT éªŒè¯ç­‰ã€‚
- **æˆæƒ (Authorization):** åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ (RBAC)ï¼Œå®šä¹‰å“ªäº›æœåŠ¡å¯ä»¥è®¿é—®å“ªäº›å…¶ä»–æœåŠ¡ã€‚

#### 3. å¯è§‚æµ‹æ€§ (Observability)

Envoy ä»£ç†ä½œä¸ºæ‰€æœ‰æµé‡çš„å¿…ç»ä¹‹è·¯ï¼Œä½¿å…¶æˆä¸ºæ”¶é›†é¥æµ‹æ•°æ®çš„å®Œç¾åˆ‡å…¥ç‚¹ï¼š

- **Metrics (æŒ‡æ ‡):** è‡ªåŠ¨æ”¶é›†æœåŠ¡é—´çš„å»¶è¿Ÿã€è¯·æ±‚é‡å’Œé”™è¯¯ç‡ï¼ˆé€šå¸¸ä¸ Prometheus é›†æˆï¼‰ã€‚
- **Tracing (é“¾è·¯è¿½è¸ª):** è‡ªåŠ¨ä¸ºæ¯ä¸ªè¯·æ±‚ç”Ÿæˆè·Ÿè¸ª Spanï¼Œå¸®åŠ©å¼€å‘è€…è¿½è¸ªè¯·æ±‚åœ¨å¾®æœåŠ¡é—´çš„å®Œæ•´è·¯å¾„ï¼ˆé€šå¸¸ä¸ Jaeger æˆ– Zipkin é›†æˆï¼‰ã€‚
- **Logging (æ—¥å¿—):** è®°å½•è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯ã€‚

---

### Istio çš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

| ä¼˜åŠ¿ (Pros) | æŒ‘æˆ˜ (Cons) |
| :--- | :--- |
| **åŠŸèƒ½å¼ºå¤§** | **å¼•å…¥å¤æ‚æ€§ï¼š** éƒ¨ç½²å’Œç»´æŠ¤ Istio æœ¬èº«å°±æ˜¯ä¸€ä¸ªå¤æ‚çš„ç³»ç»Ÿã€‚ |
| **é€æ˜åŒ–** | **èµ„æºå¼€é”€ï¼š** æ¯ä¸ª Pod å¢åŠ ä¸€ä¸ª Envoy å®¹å™¨ï¼Œä¼šå¢åŠ é›†ç¾¤çš„ CPU å’Œå†…å­˜æ¶ˆè€—ï¼ˆç§°ä¸ºâ€œSidecar ç¨â€ï¼‰ã€‚ |
| **é›¶ä»£ç ä¿®æ”¹** | **å­¦ä¹ æ›²çº¿é™¡å³­ï¼š** æ¦‚å¿µå¤šï¼ˆCRDã€Gatewayã€VirtualService ç­‰ï¼‰ï¼Œéœ€è¦ä¸“é—¨çš„å­¦ä¹ ã€‚ |
| **é«˜åº¦æ ‡å‡†åŒ–** | **æ€§èƒ½å½±å“ï¼š** è™½ç„¶ Envoy å¾ˆå¿«ï¼Œä½†é¢å¤–çš„ç½‘ç»œè·³è½¬å’Œå¤„ç†ä»ä¼šç•¥å¾®å¢åŠ å»¶è¿Ÿã€‚ |

**æ€»ç»“ï¼š** Istio æ˜¯ä¸€ä¸ªæˆç†Ÿçš„ä¼ä¸šçº§ Service Mesh è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºéœ€è¦é«˜çº§æµé‡ç®¡ç†ã€ä¸¥æ ¼å®‰å…¨æ§åˆ¶å’Œæ·±åº¦å¯è§‚æµ‹æ€§çš„å¤§è§„æ¨¡å¾®æœåŠ¡ç¯å¢ƒã€‚

---

## Kubernetes/äº‘åŸç”Ÿåˆ†å±‚æ¨¡å‹ 

æ‚¨æå‡ºçš„â€œKubernetes L1 å±‚ã€L2 å±‚â€çš„æ¦‚å¿µï¼Œå¹¶éæ¥è‡ª Kubernetes å®˜æ–¹æ–‡æ¡£çš„ä¸¥æ ¼åˆ†å±‚æ¨¡å‹ï¼ˆä¾‹å¦‚ ISO/OSI æ¨¡å‹ï¼‰ã€‚å®ƒæ›´åƒæ˜¯ä¸€ä¸ªåœ¨ **äº‘åŸç”Ÿç¤¾åŒºå’Œè¿ç»´å®è·µä¸­** çº¦å®šä¿—æˆã€ç”¨äºæè¿°**è´£ä»»è¾¹ç•Œå’Œæ•…éšœåŸŸ**çš„åˆ†å±‚æ¨¡å‹ã€‚

è¿™ç§åˆ†å±‚å¯¹äºç†è§£**è°è´Ÿè´£ç»´æŠ¤å“ªéƒ¨åˆ†**ä»¥åŠ**æ··æ²Œå·¥ç¨‹å¦‚ä½•åˆ†å±‚æµ‹è¯•**è‡³å…³é‡è¦ã€‚

---

### Kubernetes/äº‘åŸç”Ÿåˆ†å±‚æ¨¡å‹ (å®è·µçº¦å®š)

é€šå¸¸ï¼Œä¸€ä¸ªå®Œæ•´çš„äº‘åŸç”Ÿåº”ç”¨å †æ ˆå¯ä»¥ç²—ç•¥åœ°åˆ†ä¸º 3 åˆ° 4 ä¸ªå±‚çº§ï¼Œä»æœ€åº•å±‚çš„ç‰©ç†èµ„æºåˆ°æœ€ä¸Šå±‚çš„ç”¨æˆ·åº”ç”¨ï¼š

#### L1ï¼šåŸºç¡€è®¾æ–½å±‚ (The Infrastructure Layer)

- **å®šä¹‰ï¼š** æ„æˆ Kubernetes é›†ç¾¤è¿è¡ŒåŸºç¡€çš„åº•å±‚è®¡ç®—ã€ç½‘ç»œå’Œå­˜å‚¨èµ„æºã€‚è¿™ä¸€å±‚é€šå¸¸ç”±**äº‘æœåŠ¡æä¾›å•† (CSP)** æˆ–**è£¸é‡‘å±è¿ç»´å›¢é˜Ÿ**è´Ÿè´£ã€‚
- **è´£ä»»è¾¹ç•Œï¼š** è´Ÿè´£æä¾›ç¨³å®šçš„ IaaS (Infrastructure as a Service)ã€‚
- **èµ„æºç¤ºä¾‹ï¼š**
  - **è®¡ç®—ï¼š** AWS EC2 å®ä¾‹ã€GCP Compute Engineã€Azure VM (å³ K8s çš„ Worker/Control Plane èŠ‚ç‚¹)ã€‚
  - **ç½‘ç»œï¼š** VPCã€å­ç½‘ã€è·¯ç”±è¡¨ã€ç½‘ç»œäº’è”ã€Load Balancer æœåŠ¡æœ¬èº«ã€‚
  - **å­˜å‚¨ï¼š** å—å­˜å‚¨ (EBS/PD)ã€æ–‡ä»¶å­˜å‚¨ã€‚
- **æ•…éšœæ³¨å…¥å·¥å…·ï¼š** **AWS FIS**ã€äº‘å¹³å° APIã€‚
- **ç®¡ç†å·¥å…·ï¼š** **Terraform**ã€CloudFormationã€‚

#### L2ï¼šKubernetes æ§åˆ¶å±‚/å¹³å°å±‚ (The Control Plane & Platform Layer)

- **å®šä¹‰ï¼š** Kubernetes ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£é›†ç¾¤çš„è‡ªåŠ¨åŒ–ã€è°ƒåº¦å’Œè‡ªæ„ˆã€‚è¿™ä¸€å±‚é€šå¸¸ç”±**å¹³å°å·¥ç¨‹ (Platform Engineering) å›¢é˜Ÿ**æˆ–äº‘æœåŠ¡å•†è´Ÿè´£ã€‚
- **è´£ä»»è¾¹ç•Œï¼š** è´Ÿè´£é›†ç¾¤çš„å¥åº·è¿è¡Œå’Œé«˜æ•ˆè°ƒåº¦ã€‚
- **èµ„æºç¤ºä¾‹ï¼š**
  - **æ§åˆ¶å¹³é¢ç»„ä»¶ï¼š** API Server, etcd, Scheduler, Controller Managerã€‚
  - **èŠ‚ç‚¹ä»£ç†ï¼š** Kubelet, Kube-proxyã€‚
  - **é›†ç¾¤æ’ä»¶ï¼š** CNI (ç½‘ç»œæ’ä»¶), CSI (å­˜å‚¨æ’ä»¶), CoreDNSã€‚
- **æ•…éšœæ³¨å…¥å·¥å…·ï¼š** **Chaos Mesh**ã€**LitmusChaos** (é’ˆå¯¹ Kubelet/Kube-proxy/CNI)ã€‚
- **ç®¡ç†å·¥å…·ï¼š** **Kubeadm**ã€`eksctl`ã€Helmã€‚

#### L3ï¼šåº”ç”¨æœåŠ¡å±‚ (The Application Services Layer)

- **å®šä¹‰ï¼š** è¿è¡Œåœ¨ Kubernetes ä¸Šçš„ç”¨æˆ·åº”ç”¨ç¨‹åºå’Œå®ƒä»¬çš„é…ç½®ã€‚è¿™ä¸€å±‚ä¸»è¦ç”±**å¼€å‘ (Dev) æˆ– SRE å›¢é˜Ÿ**è´Ÿè´£ã€‚
- **è´£ä»»è¾¹ç•Œï¼š** è´Ÿè´£åº”ç”¨çš„æ­£ç¡®æ€§ã€éƒ¨ç½²ã€å¼¹æ€§å’Œå¯è§‚æµ‹æ€§ã€‚
- **èµ„æºç¤ºä¾‹ï¼š**
  - **Kubernetes å¯¹è±¡ï¼š** Deployment, Service, ConfigMap, Secret, Ingressã€‚
  - **ä¸šåŠ¡ä»£ç ï¼š** å¾®æœåŠ¡ã€æ•°æ®åº“å®¢æˆ·ç«¯ã€ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚
- **æ•…éšœæ³¨å…¥å·¥å…·ï¼š** **LitmusChaos** (åˆ©ç”¨å…¶ Probes/Workflows è¿›è¡Œä¸šåŠ¡éªŒè¯)ã€**Chaos Mesh** (æ³¨å…¥ Pod çº§çš„èµ„æºå‹åŠ›)ã€‚
- **ç®¡ç†å·¥å…·ï¼š** **Helm**ã€**ArgoCD/FluxCD (GitOps)**ã€‚

#### L4ï¼šç”¨æˆ·ä½“éªŒå±‚ (The End-User Experience Layer)

- **å®šä¹‰ï¼š** ç”¨æˆ·ç›´æ¥æ„Ÿå—åˆ°çš„åº”ç”¨ç¨‹åºæ€§èƒ½å’ŒåŠŸèƒ½ã€‚
- **è´£ä»»è¾¹ç•Œï¼š** æœ€ç»ˆçš„ä¸šåŠ¡æŒ‡æ ‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚
- **æŒ‡æ ‡ç¤ºä¾‹ï¼š** å“åº”å»¶è¿Ÿã€äº‹åŠ¡æˆåŠŸç‡ã€SLA/SLO è¾¾æˆæƒ…å†µã€‚

---

### ğŸ“œ å®˜æ–¹æ–‡æ¡£è¯´æ˜

å¦‚å‰æ‰€è¿°ï¼ŒKubernetes **å®˜æ–¹æ–‡æ¡£** å¹¶æ²¡æœ‰ä¸€ä¸ªåä¸º L1/L2/L3 çš„åˆ†å±‚æ¨¡å‹ã€‚

å®˜æ–¹æ–‡æ¡£æ›´å¤šåœ°ä½¿ç”¨**ç»„ä»¶ (Components)** å’Œ**æ¦‚å¿µ (Concepts)** æ¥æè¿°ç³»ç»Ÿç»“æ„ã€‚æœ€æ¥è¿‘çš„åˆ†å±‚æ¨¡å‹æ˜¯ï¼š

1. **æ§åˆ¶å¹³é¢ç»„ä»¶ (Control Plane Components):** å¯¹åº”äºæˆ‘ä»¬çš„ L2 å±‚æ ¸å¿ƒã€‚
2. **å·¥ä½œèŠ‚ç‚¹ç»„ä»¶ (Node Components):** å¯¹åº”äº L1 ä¸Šçš„ Kubelet å’Œ Kube-proxyã€‚
3. **å¯¹è±¡ (Objects) å’Œæ¦‚å¿µ:** å¯¹åº”äº L3 å±‚çš„èµ„æºå®šä¹‰ã€‚

æ‚¨å¯ä»¥å‚è€ƒ Kubernetes å®˜æ–¹æ–‡æ¡£ä¸­çš„ **â€œKubernetes åŸºç¡€æ¦‚å¿µâ€** å’Œ **â€œæ¶æ„â€** éƒ¨åˆ†æ¥äº†è§£å®˜æ–¹çš„ç»“æ„æè¿°ã€‚ 